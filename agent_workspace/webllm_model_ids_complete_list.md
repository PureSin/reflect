# Complete List of Available WebLLM Model IDs

## Overview

This report contains the complete list of available WebLLM model IDs extracted from the official MLC-AI WebLLM repository. The information was gathered from:

1. **GitHub Issue #683**: "List of currently available models" - https://github.com/mlc-ai/web-llm/issues/683
2. **Authoritative Source**: `config.ts` file - https://github.com/mlc-ai/web-llm/blob/main/src/config.ts#L293

**Current Model Version**: `v0_2_48`  
**Model Library URL Prefix**: `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/`

## Complete List of WebLLM Model IDs

### Llama-3.2 Models
- `Llama-3.2-1B-Instruct-q4f32_1-MLC`
- `Llama-3.2-1B-Instruct-q4f16_1-MLC`
- `Llama-3.2-1B-Instruct-q0f32-MLC`
- `Llama-3.2-1B-Instruct-q0f16-MLC`
- `Llama-3.2-3B-Instruct-q4f32_1-MLC`
- `Llama-3.2-3B-Instruct-q4f16_1-MLC`

### Llama-3.1 Models
- `Llama-3.1-8B-Instruct-q4f32_1-MLC-1k`
- `Llama-3.1-8B-Instruct-q4f16_1-MLC-1k`
- `Llama-3.1-8B-Instruct-q4f32_1-MLC`
- `Llama-3.1-8B-Instruct-q4f16_1-MLC`
- `Llama-3.1-70B-Instruct-q3f16_1-MLC`

### Llama-3 Models
- `Llama-3-8B-Instruct-q4f32_1-MLC-1k`
- `Llama-3-8B-Instruct-q4f16_1-MLC-1k`
- `Llama-3-8B-Instruct-q4f32_1-MLC`
- `Llama-3-8B-Instruct-q4f16_1-MLC`
- `Llama-3-70B-Instruct-q3f16_1-MLC`

### Llama-2 Models
- `Llama-2-7b-chat-hf-q4f32_1-MLC-1k`
- `Llama-2-7b-chat-hf-q4f16_1-MLC-1k`
- `Llama-2-7b-chat-hf-q4f32_1-MLC`
- `Llama-2-7b-chat-hf-q4f16_1-MLC`
- `Llama-2-13b-chat-hf-q4f16_1-MLC`

### DeepSeek-R1-Distill Models
#### Qwen Base
- `DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC`
- `DeepSeek-R1-Distill-Qwen-7B-q4f32_1-MLC`

#### Llama Base
- `DeepSeek-R1-Distill-Llama-8B-q4f32_1-MLC`
- `DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC`

### Hermes Models (Llama Base)
- `Hermes-2-Theta-Llama-3-8B-q4f16_1-MLC`
- `Hermes-2-Theta-Llama-3-8B-q4f32_1-MLC`
- `Hermes-2-Pro-Llama-3-8B-q4f16_1-MLC`
- `Hermes-2-Pro-Llama-3-8B-q4f32_1-MLC`
- `Hermes-3-Llama-3.2-3B-q4f32_1-MLC`
- `Hermes-3-Llama-3.2-3B-q4f16_1-MLC`
- `Hermes-3-Llama-3.1-8B-q4f32_1-MLC`
- `Hermes-3-Llama-3.1-8B-q4f16_1-MLC`

### Hermes Models (Mistral Base)
- `Hermes-2-Pro-Mistral-7B-q4f16_1-MLC`

### Phi-3.5 Models
- `Phi-3.5-mini-instruct-q4f16_1-MLC`
- `Phi-3.5-mini-instruct-q4f32_1-MLC`
- `Phi-3.5-mini-instruct-q4f16_1-MLC-1k`
- `Phi-3.5-mini-instruct-q4f32_1-MLC-1k`
- `Phi-3.5-vision-instruct-q4f16_1-MLC` (VLM - Vision-Language Model)
- `Phi-3.5-vision-instruct-q4f32_1-MLC` (VLM - Vision-Language Model)

### Phi-3 Models
- `Phi-3-mini-4k-instruct-q4f16_1-MLC`
- `Phi-3-mini-4k-instruct-q4f32_1-MLC`
- `Phi-3-mini-4k-instruct-q4f16_1-MLC-1k`
- `Phi-3-mini-4k-instruct-q4f32_1-MLC-1k`

### Phi-2 and Phi-1.5 Models
- `phi-2-q4f16_1-MLC`
- `phi-2-q4f32_1-MLC`
- `phi-2-q4f16_1-MLC-1k`
- `phi-2-q4f32_1-MLC-1k`
- `phi-1_5-q4f16_1-MLC`
- `phi-1_5-q4f32_1-MLC`
- `phi-1_5-q4f16_1-MLC-1k`
- `phi-1_5-q4f32_1-MLC-1k`

### Mistral Models
- `Mistral-7B-Instruct-v0.3-q4f16_1-MLC`
- `Mistral-7B-Instruct-v0.3-q4f32_1-MLC`
- `Mistral-7B-Instruct-v0.2-q4f16_1-MLC`
- `OpenHermes-2.5-Mistral-7B-q4f16_1-MLC`
- `NeuralHermes-2.5-Mistral-7B-q4f16_1-MLC`
- `WizardMath-7B-V1.1-q4f16_1-MLC`

### SmolLM2 Models
- `SmolLM2-1.7B-Instruct-q4f16_1-MLC`
- `SmolLM2-1.7B-Instruct-q4f32_1-MLC`
- `SmolLM2-360M-Instruct-q0f16-MLC`
- `SmolLM2-360M-Instruct-q0f32-MLC`
- `SmolLM2-360M-Instruct-q4f16_1-MLC`
- `SmolLM2-360M-Instruct-q4f32_1-MLC`
- `SmolLM2-135M-Instruct-q0f16-MLC`
- `SmolLM2-135M-Instruct-q0f32-MLC`

### Gemma-2 Models
- `gemma-2-2b-it-q4f16_1-MLC`
- `gemma-2-2b-it-q4f32_1-MLC`
- `gemma-2-2b-it-q4f16_1-MLC-1k`
- `gemma-2-2b-it-q4f32_1-MLC-1k`
- `gemma-2-9b-it-q4f16_1-MLC`
- `gemma-2-9b-it-q4f32_1-MLC`
- `gemma-2-2b-jpn-it-q4f16_1-MLC` (Japanese)
- `gemma-2-2b-jpn-it-q4f32_1-MLC` (Japanese)

### Original Gemma Models
- `gemma-2b-it-q4f16_1-MLC`
- `gemma-2b-it-q4f32_1-MLC`
- `gemma-2b-it-q4f16_1-MLC-1k`
- `gemma-2b-it-q4f32_1-MLC-1k`

### Qwen-2.5 Models
- `Qwen2.5-0.5B-Instruct-q4f16_1-MLC`
- `Qwen2.5-0.5B-Instruct-q4f32_1-MLC`
- `Qwen2.5-0.5B-Instruct-q0f16-MLC`
- `Qwen2.5-0.5B-Instruct-q0f32-MLC`
- `Qwen2.5-1.5B-Instruct-q4f16_1-MLC`
- `Qwen2.5-1.5B-Instruct-q4f32_1-MLC`
- `Qwen2.5-3B-Instruct-q4f16_1-MLC`
- `Qwen2.5-3B-Instruct-q4f32_1-MLC`
- `Qwen2.5-7B-Instruct-q4f16_1-MLC`
- `Qwen2.5-7B-Instruct-q4f32_1-MLC`

### Qwen-2.5-Coder Models
- `Qwen2.5-Coder-0.5B-Instruct-q4f16_1-MLC`
- `Qwen2.5-Coder-0.5B-Instruct-q4f32_1-MLC`
- `Qwen2.5-Coder-0.5B-Instruct-q0f16-MLC`
- `Qwen2.5-Coder-0.5B-Instruct-q0f32-MLC`
- `Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC`
- `Qwen2.5-Coder-1.5B-Instruct-q4f32_1-MLC`
- `Qwen2.5-Coder-3B-Instruct-q4f16_1-MLC`
- `Qwen2.5-Coder-3B-Instruct-q4f32_1-MLC`
- `Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC`
- `Qwen2.5-Coder-7B-Instruct-q4f32_1-MLC`

### Qwen-2.5-Math Models
- `Qwen2.5-Math-1.5B-Instruct-q4f16_1-MLC`
- `Qwen2.5-Math-1.5B-Instruct-q4f32_1-MLC`

### Qwen-2 Models
- `Qwen2-0.5B-Instruct-q4f16_1-MLC`
- `Qwen2-0.5B-Instruct-q0f16-MLC`
- `Qwen2-0.5B-Instruct-q0f32-MLC`
- `Qwen2-1.5B-Instruct-q4f16_1-MLC`
- `Qwen2-1.5B-Instruct-q4f32_1-MLC`
- `Qwen2-7B-Instruct-q4f16_1-MLC`
- `Qwen2-7B-Instruct-q4f32_1-MLC`

### Qwen-2-Math Models
- `Qwen2-Math-1.5B-Instruct-q4f16_1-MLC`
- `Qwen2-Math-1.5B-Instruct-q4f32_1-MLC`
- `Qwen2-Math-7B-Instruct-q4f16_1-MLC`
- `Qwen2-Math-7B-Instruct-q4f32_1-MLC`

### StableLM Models
- `stablelm-2-zephyr-1_6b-q4f16_1-MLC`
- `stablelm-2-zephyr-1_6b-q4f32_1-MLC`
- `stablelm-2-zephyr-1_6b-q4f16_1-MLC-1k`
- `stablelm-2-zephyr-1_6b-q4f32_1-MLC-1k`

### RedPajama Models
- `RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC`
- `RedPajama-INCITE-Chat-3B-v1-q4f32_1-MLC`
- `RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC-1k`
- `RedPajama-INCITE-Chat-3B-v1-q4f32_1-MLC-1k`

### TinyLlama Models
#### Version 1.0
- `TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC`
- `TinyLlama-1.1B-Chat-v1.0-q4f32_1-MLC`
- `TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC-1k`
- `TinyLlama-1.1B-Chat-v1.0-q4f32_1-MLC-1k`

#### Version 0.4
- `TinyLlama-1.1B-Chat-v0.4-q4f16_1-MLC`
- `TinyLlama-1.1B-Chat-v0.4-q4f32_1-MLC`
- `TinyLlama-1.1B-Chat-v0.4-q4f16_1-MLC-1k`
- `TinyLlama-1.1B-Chat-v0.4-q4f32_1-MLC-1k`

### Embedding Models
- `snowflake-arctic-embed-m-q0f32-MLC-b32`
- `snowflake-arctic-embed-m-q0f32-MLC-b4`
- `snowflake-arctic-embed-s-q0f32-MLC-b32`
- `snowflake-arctic-embed-s-q0f32-MLC-b4`

## Function Calling Support

The following models support function calling capabilities:
- `Hermes-2-Pro-Llama-3-8B-q4f16_1-MLC`
- `Hermes-2-Pro-Llama-3-8B-q4f32_1-MLC`
- `Hermes-2-Pro-Mistral-7B-q4f16_1-MLC`
- `Hermes-3-Llama-3.1-8B-q4f32_1-MLC`
- `Hermes-3-Llama-3.1-8B-q4f16_1-MLC`

## Model Naming Convention

WebLLM model IDs follow this general pattern:
```
{ModelFamily}-{Version}-{Size}-{Variant}-{Quantization}-MLC[-{ContextSize}]
```

Where:
- **ModelFamily**: Llama, Phi, Mistral, Qwen, etc.
- **Version**: Version number (e.g., 3.2, 3.1, 2.5)
- **Size**: Model size (1B, 3B, 7B, 8B, etc.)
- **Variant**: Instruct, Chat, etc.
- **Quantization**: 
  - `q4f32_1`: 4-bit quantization with 32-bit floating point
  - `q4f16_1`: 4-bit quantization with 16-bit floating point
  - `q0f32`: Full precision 32-bit
  - `q0f16`: Full precision 16-bit
  - `q3f16_1`: 3-bit quantization with 16-bit floating point
- **ContextSize**: Optional suffix (e.g., `-1k` for 1024 context window)

## Key Features

- **VRAM Requirements**: Range from 359.69 MB (SmolLM2-135M) to 8383.33 MB (gemma-2-9b)
- **Context Windows**: Typically 1024 or 4096 tokens
- **Low Resource Options**: Many models marked as low-resource compatible
- **Vision Support**: Phi-3.5-vision models support multimodal input
- **Specialized Models**: Math, coding, and embedding variants available
- **Multiple Languages**: Japanese variants available for Gemma models

## Data Sources

- **Primary Source**: https://github.com/mlc-ai/web-llm/blob/main/src/config.ts#L293
- **Reference Documentation**: https://github.com/mlc-ai/web-llm/issues/683
- **Model Version**: v0_2_48 (as of extraction date: 2025-08-22)

---

*This list represents the complete set of available WebLLM model IDs as extracted from the official MLC-AI WebLLM repository configuration files and documentation.*