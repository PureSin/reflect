{"extracted_information": "SmolLM is a family of state-of-the-art small language models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset called SmolLM-Corpus. They are designed for local device operation, reducing inference costs and improving user privacy. The models demonstrate strong performance compared to other models in their size categories across various benchmarks, including reasoning, common sense, world knowledge, and coding. They are suitable for deployment on a wide range of devices, including smartphones and laptops, with specific memory footprints for each size, and are available with WebGPU demos for browser-based AI applications.", "specifications": {"model_family": "SmolLM", "size_variants": [{"parameters": "135M", "training_tokens": "600B", "architecture": {"design_philosophy": "Similar to MobileLLM, prioritizing depth over width", "attention_mechanism": "Grouped-Query Attention (GQA)", "embedding_tying": true, "context_length": "2048 tokens (extensible with long context fine-tuning)", "vocab_size": "49152", "layers": 12, "attention_heads": 8, "hidden_size": 768, "intermediate_size": 2048, "num_key_value_heads": 1}, "memory_footprint": "260MB"}, {"parameters": "360M", "training_tokens": "600B", "architecture": {"design_philosophy": "Similar to MobileLLM, prioritizing depth over width", "attention_mechanism": "Grouped-Query Attention (GQA)", "embedding_tying": true, "context_length": "2048 tokens (extensible with long context fine-tuning)", "vocab_size": "49152", "layers": 24, "attention_heads": 16, "hidden_size": 1024, "intermediate_size": 2736, "num_key_value_heads": 2}, "memory_footprint": "680MB"}, {"parameters": "1.7B", "training_tokens": "1T", "architecture": {"design_philosophy": "Traditional architecture", "embedding_tying": true, "context_length": "2048 tokens (extensible with long context fine-tuning)", "vocab_size": "49152", "layers": 24, "attention_heads": 32, "hidden_size": 2048, "intermediate_size": 5504, "num_key_value_heads": 8}, "memory_footprint": "3.3GB"}], "training_corpus": {"name": "SmolLM-Corpus", "components": [{"name": "Cosmopedia v2", "description": "Enhanced version of Cosmopedia, 28B tokens of synthetic textbooks, stories, articles, and code generated by Mixtral (previously Mixtral-8x7B-Instruct-v0.1, experimented with Llama3-70B-Instruct, Mixtral-8x22B-Instruct-v0.1, Qwen1.5-72B-Chat). Features a new prompt optimization strategy using BISAC book classification (34,000 topics) and a search tool for seed samples from FineWeb CC-MAIN-2024-10 and CC-MAIN-2023-50 dumps. Includes 40% middle school audience, 30% college audience, 30% mixed styles, and 1B code textbooks based on Python seed samples from AutoMathText."}, {"name": "FineWeb-Edu (deduplicated)", "description": "220B deduplicated tokens from FineWeb, filtered using an educational quality classifier trained with Llama3-70B-Instruct annotations."}, {"name": "Python-Edu", "description": "4B tokens of refined Python samples from The Stack dataset, filtered using an educational code classifier trained with Llama3 annotations. Converges 3x faster than unfiltered Python code."}]}}, "pricing": {}, "features": ["State-of-the-art small language models (SLMs)", "Designed for local device operation", "Reduces inference costs", "Improves user privacy", "Trained on meticulously curated high-quality training corpus (SmolLM-Corpus)", "Outperforms other models in their size categories on diverse benchmarks", "Models available as Transformers checkpoints", "ONNX checkpoints available", "GGUF version planned (compatible with `llama.cpp`)"], "statistics": {"performance_benchmarks": {"evaluation_setup": "Same setup for all models using `lighteval` library from `huggingface/cosmopedia/tree/main/evaluation`. HumanEval uses `bigcode-evaluation-harness` with temperature 0.2, top-p 0.95, 20 samples.", "comparative_performance": [{"model": "SmolLM-135M", "details": "Outperforms MobileLM-125M (current best model <200M params), despite training on 600B vs MobileLM's 1T tokens."}, {"model": "SmolLM-360M", "details": "Outperforms all models <500M params, despite having fewer parameters and training on 600B vs MobileLM-350M and Qwen2-500M."}, {"model": "SmolLM-1.7B", "details": "Outperforms all other models <2B params, including Phi1.5, MobileLM-1.5B, and Qwen2-1.5B. Shows strong Python coding performance with 24 pass@1 (HumanEval)."}], "instruct_tuned_performance": {"methodology": "Instruction tuned using publicly available permissive instruction datasets. Trained for one epoch on permissive subset of WebInstructSub combined with StarCoder2-Self-OSS-Instruct. Followed by DPO for one epoch (HelpSteer for 135M/1.7B, argilla/dpo-mix-7k for 360M) using Zephyr-Gemma recipe with SFT learning rate 3e-4.", "IFEval_benchmark_prompt_strict_accuracy": {"SmolLM-Instruct_models": "Provide a good balance between model size and performance using only publicly available permissive datasets.", "Qwen2-1.5B-Instruct": "29.94 (highest score)"}}}}, "temporal_info": {}, "geographical_data": {}, "references": ["SmolLM models collection: https://huggingface.co/collections/HuggingFaceTB/smollm-models-6695016cad7167254ce15966", "SmolLM-Corpus dataset: https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus", "WebGPU demo: (links not rendered fully in content, but mentioned)", "Cosmopedia v2: https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus", "Cosmopedia blog post: https://huggingface.co/blog/cosmopedia", "BISAC book classification: https://www.bisg.org/complete-bisac-subject-headings-list", "Cosmopedia search tool code: https://github.com/huggingface/cosmopedia/tree/main/fulltext_search", "FineWeb technical report: https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1", "FineWeb-Edu classifier: https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier", "Educational code classifier: https://huggingface.co/HuggingFaceTB/python-edu-scorer", "MobileLLM paper: https://arxiv.org/abs/2402.14905", "HÃ¤gele et al. (trapezoidal scheduler): https://arxiv.org/pdf/2405.18392", "Evaluation setup code: https://github.com/huggingface/cosmopedia/tree/main/evaluation", "WebInstructSub dataset: https://huggingface.co/datasets/TIGER-Lab/WebInstructSub", "HelpSteer dataset: https://huggingface.co/datasets/nvidia/HelpSteer", "argilla/dpo-mix-7k dataset: https://huggingface.co/datasets/argilla/dpo-mix-7k", "Alignment Handbook (Zephyr-Gemma recipe): https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-gemma/README.md"], "deployment_characteristics": {"local_execution": "Designed to run locally on various hardware configurations.", "hardware_suitability": "Suitable for deployment on a wide range of devices, from smartphones to laptops.", "examples": "iPhone 15 (6GB DRAM), iPhone 15 Pro (8GB DRAM).", "available_formats": ["Transformers checkpoints", "ONNX checkpoints", "GGUF (planned, compatible with `llama.cpp`)"], "browser_based_ai_suitability": "Explicitly demonstrated through WebGPU demos for SmolLM-135M and Smol-LM360M, indicating direct browser-based AI application potential."}, "sentiment_analysis_capabilities": "No explicit mention of sentiment analysis capabilities for the SmolLM models in the provided content."}