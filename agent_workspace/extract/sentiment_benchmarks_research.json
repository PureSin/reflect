{"extracted_information": "This paper presents a targeted distillation framework for sentiment analysis, enabling compact models (small language models) to achieve strong sentiment analysis capabilities by leveraging knowledge and task alignment from advanced large language models (LLMs). The methodology involves a two-stage distillation framework: Knowledge-Driven Distillation (KnowDist) for transferring fundamental sentiment knowledge and In-Context Learning Distillation (ICLDist) for optimizing task-specific prompt-following abilities. The research introduces SentiBench, a comprehensive sentiment analysis benchmark, for evaluation. Experimental results demonstrate that the proposed model (a distilled Llama-3-1.2B) significantly outperforms generic distillation methods, achieves an average F1-score improvement of over 10% on SentiBench, and exhibits strong competitiveness against other small-scale LLMs, even surpassing a larger Llama-3-3.2B model. The study also validates the necessity and complementary nature of the two distillation stages and shows that the approach does not degrade general language understanding abilities (MMLU).", "specifications": {"model_architecture": "Targeted distillation of Llama-3-1.2B (student) from Llama-3.1-70B-Instruct (teacher)", "distillation_framework": "Two-stage: KnowDist and ICLDist", "knowdist_details": {"objective": "Transfer fundamental sentiment analysis capabilities", "prompting_methods": ["Analyzing", "Rewriting"], "prompting_strategy": "Multi-Perspective Prompting (MPP) covering Expression, Target, Emotion, Background", "data_sources": ["IMDb", "Yelp", "Amazon", "Twitter"], "sample_volume": "1 million samples"}, "icldist_details": {"objective": "Transfer task-specific prompt-following abilities (task alignment)", "distillation_tasks": ["Sentiment Classification", "Emotion Recognition"], "diversification_strategies": ["Format diversification (Label Word formats, Label Taxonomies, Minimized Instructions)", "Task diversification (100 natural language understanding tasks from Super-NaturalInstructions, excluding sentiment analysis)"], "data_sources": ["IMDb", "Yelp", "Amazon", "Twitter", "Super-NaturalInstructions"], "sample_volume": "400K ICLDist samples + 100K general task samples"}, "evaluation_setup": {"benchmark": "SentiBench (3 task categories, 12 datasets)", "in_context_learning_setting": "4 demonstrations randomly sampled from training set", "inference_parameters": "Temperature = 0", "random_seeds": "3 different random seeds, averaged results"}, "teacher_model": "Llama-3.1-70B-Instruct", "student_model": "Llama-3.2-1.2B-Instruct", "baselines_and_reference_models": ["Generic Distillation: Llama-3-1.2B with Alpaca-data (52K) and Lamini-data (2.58M)", "Fine-tuned T5-base", "Llama-3 family: Llama-3-3.2B, Llama-3-8B, Llama-3-70B", "Small-scale models (1B to 3B parameters): OPT-1.3B, TinyLlama-1.1B-Chat-v1.0, Phi-2-2.7B, Qwen-2.5-1.5B-Instruct, Gemma-2-2.6B-it", "GPT-3.5"], "hyperparameters": {"knowdist_optimization": {"batch_size": 128, "learning_rate": "5e-6", "training_epoch": 4, "learning_rate_decay": "Cosine", "warmup_step_ratio": 0.01, "weight_decay": 0.1, "adam_beta1": 0.9, "adam_beta2": 0.95}, "icldist_optimization": {"batch_size": 128, "learning_rate": "1e-5", "training_epoch": 4, "learning_rate_decay": "Linear", "warmup_step_ratio": 0.02, "weight_decay": 0.01, "adam_beta1": 0.9, "adam_beta2": 0.999}}}, "pricing": {}, "features": [{"name": "Targeted Distillation Framework", "description": "A novel two-stage approach for transferring sentiment analysis capabilities from large LLMs to smaller models, decoupling distillation into knowledge acquisition and task alignment."}, {"name": "Knowledge-Driven Distillation (KnowDist)", "description": "Enhances fundamental sentiment analysis capabilities by transferring sentiment-related knowledge using multi-perspective prompting (analyzing and rewriting texts focusing on expression, target, emotion, and background)."}, {"name": "In-Context Learning Distillation (ICLDist)", "description": "Optimizes task-specific prompt-following abilities and improves generalization to unseen tasks by mimicking teacher LLM responses on few-shot samples, employing format and task diversification strategies."}, {"name": "SentiBench Benchmark", "description": "A comprehensive sentiment analysis benchmark for systematically assessing LLM capabilities, covering three categories: Basic Sentiment Analysis (BSA), Multifaceted Sentiment Analysis (MSA), and Fine-Grained Sentiment Analysis (FSA), across 12 datasets."}, {"name": "Multi-Perspective Prompting (MPP)", "description": "A strategy within KnowDist that guides analysis and rewriting from four perspectives (Expression, Target, Emotion, Background) to elicit comprehensive sentiment-related knowledge."}, {"name": "Diversification Strategies (ICLDist)", "description": "Strategies (Label Word, Label Taxonomies, Minimized Instructions for format; varied NLU tasks for task) to enhance generalization of the student model to unseen tasks during in-context learning."}], "statistics": {"sentibench_tasks_and_metrics": {"basic_sentiment_analysis": {"description": "Classify overall sentiment polarity (document-level and sentence-level)", "datasets": {"imdb": {"train": 3000, "dev": 300, "test": 1000, "classes": 2, "metric": "macro_f1"}, "yelp2": {"train": 3000, "dev": 300, "test": 1000, "classes": 2, "metric": "macro_f1"}, "sst2": {"train": 3000, "dev": 300, "test": 1821, "classes": 2, "metric": "macro_f1"}, "twitter17": {"train": 3000, "dev": 300, "test": 1000, "classes": 3, "metric": "macro_f1"}}}, "multifaceted_sentiment_analysis": {"description": "Recognize broader range of human emotional states", "datasets": {"irony18": {"train": 3000, "dev": 300, "test": 784, "classes": 2, "metric": "macro_f1"}, "emotion20": {"train": 3000, "dev": 300, "test": 1421, "classes": 4, "metric": "macro_f1"}, "p_stance": {"train": 3000, "dev": 300, "test": 2157, "classes": 3, "metric": "macro_f1"}, "mint_english": {"train": 1287, "dev": 300, "test": 396, "classes": 3, "metric": "macro_f1"}}}, "fine_grained_sentiment_analysis": {"description": "Recognize spectrum of sentiment elements", "datasets": {"rest16": {"train": "1600/1264", "dev": "400/316", "test": "676/544", "classes": "-", "metric": "micro_f1", "tasks": ["ATSA", "ACSA", "ASQP"]}, "opener": {"train": 1744, "dev": 249, "test": 499, "classes": "-", "metric": "micro_f1", "tasks": ["SSA"]}}}}, "performance_comparisons": {"table_2_experimental_results_f1_score": {"unit": "%", "models": {"t5_base_fine_tuned": {"average": 74.24, "imdb": 92.8, "yelp2": 96.62, "sst2": 92.2, "twitter": 65.95, "irony": 75.18, "emotion": 79.45, "stance": 72.51, "intim": 52.43, "atsa": 74.19, "acsa": 72.42, "asqp": 59.45, "ssa": 57.63}, "llama_3_3_2b": {"average": 59.5, "imdb": 92.57, "yelp2": 96.53, "sst2": 93.59, "twitter": 61.45, "irony": 64.0, "emotion": 68.88, "stance": 71.43, "intim": 33.32, "atsa": 46.37, "acsa": 51.66, "asqp": 11.09, "ssa": 23.1}, "llama_3_8b": {"average": 67.22, "imdb": 94.17, "yelp2": 98.07, "sst2": 95.9, "twitter": 66.58, "irony": 82.63, "emotion": 73.0, "stance": 75.86, "intim": 49.85, "atsa": 54.41, "acsa": 64.57, "asqp": 19.67, "ssa": 31.91}, "llama_3_70b": {"average": 72.78, "imdb": 95.3, "yelp2": 98.1, "sst2": 97.14, "twitter": 68.75, "irony": 83.99, "emotion": 75.87, "stance": 85.21, "intim": 53.68, "atsa": 63.78, "acsa": 75.21, "asqp": 31.03, "ssa": 45.29}, "gpt_3_5": {"average": 63.79, "imdb": 93.4, "yelp2": 97.5, "sst2": 93.57, "twitter": 67.55, "irony": 65.25, "emotion": 78.14, "stance": 75.84, "intim": 55.82, "atsa": 39.96, "acsa": 64.58, "asqp": 30.42, "ssa": 3.41}, "opt_1_3b": {"average": 42.05, "imdb": 78.94, "yelp2": 91.37, "sst2": 77.1, "twitter": 39.32, "irony": 51.18, "emotion": 43.98, "stance": 53.93, "intim": 32.65, "atsa": 11.39, "acsa": 19.06, "asqp": 1.72, "ssa": 3.92}, "tinylama_1_1b": {"average": 45.18, "imdb": 71.27, "yelp2": 84.13, "sst2": 78.01, "twitter": 34.21, "irony": 56.15, "emotion": 50.05, "stance": 57.25, "intim": 36.95, "atsa": 26.76, "acsa": 29.42, "asqp": 4.24, "ssa": 13.68}, "phi_2_2_7b": {"average": 52.13, "imdb": 87.03, "yelp2": 96.1, "sst2": 90.63, "twitter": 59.59, "irony": 47.52, "emotion": 45.53, "stance": 55.36, "intim": 31.61, "atsa": 39.71, "acsa": 46.54, "asqp": 9.6, "ssa": 16.31}, "qwen_2_5_1_5b": {"average": 58.29, "imdb": 91.92, "yelp2": 97.3, "sst2": 92.33, "twitter": 52.39, "irony": 65.8, "emotion": 63.61, "stance": 70.9, "intim": 35.73, "atsa": 37.66, "acsa": 53.25, "asqp": 18.47, "ssa": 20.08}, "gemma_2_2_6b": {"average": 62.62, "imdb": 92.39, "yelp2": 97.4, "sst2": 94.17, "twitter": 56.02, "irony": 70.68, "emotion": 68.85, "stance": 73.99, "intim": 42.57, "atsa": 48.0, "acsa": 50.27, "asqp": 18.03, "ssa": 39.08}, "llama_3_1_2b_base": {"average": 50.44, "imdb": 87.65, "yelp2": 94.8, "sst2": 88.93, "twitter": 58.78, "irony": 35.8, "emotion": 58.07, "stance": 60.78, "intim": 25.6, "atsa": 33.8, "acsa": 36.09, "asqp": 8.05, "ssa": 16.91}, "llama_3_1_2b_distill_alpaca_data": {"average": "51.25(+0.81)", "imdb": 89.13, "yelp2": 94.37, "sst2": 91.08, "twitter": 58.02, "irony": 33.01, "emotion": 60.24, "stance": 64.02, "intim": 26.1, "atsa": 36.18, "acsa": 37.71, "asqp": 8.72, "ssa": 16.44}, "llama_3_1_2b_distill_lamini_data": {"average": "51.87(+1.43)", "imdb": 89.26, "yelp2": 94.63, "sst2": 91.14, "twitter": 62.9, "irony": 38.05, "emotion": 50.61, "stance": 63.92, "intim": 27.9, "atsa": 35.03, "acsa": 41.89, "asqp": 8.3, "ssa": 18.8}, "llama_3_1_2b_knowdist": {"average": "54.03(+3.59)", "imdb": 88.53, "yelp2": 95.37, "sst2": 90.8, "twitter": 61.54, "irony": 44.01, "emotion": 63.49, "stance": 63.59, "intim": 31.11, "atsa": 38.75, "acsa": 41.2, "asqp": 10.3, "ssa": 19.62}, "llama_3_1_2b_icldist": {"average": "59.72(+9.28)", "imdb": 92.9, "yelp2": 97.63, "sst2": 94.51, "twitter": 68.91, "irony": 65.35, "emotion": 76.27, "stance": 70.17, "intim": 35.15, "atsa": 37.71, "acsa": 48.06, "asqp": 9.76, "ssa": 20.16}, "llama_3_1_2b_know_icldist": {"average": "60.77(+10.33)", "imdb": 93.07, "yelp2": 97.7, "sst2": 94.53, "twitter": 68.37, "irony": 73.8, "emotion": 76.79, "stance": 69.94, "intim": 35.39, "atsa": 39.01, "acsa": 47.82, "asqp": 11.69, "ssa": 21.18}}, "key_performance_gains": {"distillation_vs_generic": "Our approach (Know & ICLDist) yields average improvement of over 10% F1-score compared to Llama-3-1.2B base.", "irony_detection_gain": "Most striking improvement: F1-score increases dramatically from 35.80% to 73.80% (+38.00%) in irony detection.", "llama_3_1_2b_vs_3_2b": "Llama-3-1.2B (Know & ICLDist) outperforms Llama-3-3.2B (60.77% vs 59.50% avg F1-score)."}}, "table_3_fine_tuning_settings_f1_score": {"unit": "%", "llama_3_1_2b": {"msa": 73.61, "fsa": 68.78}, "llama_3_1_2b_knowdist": {"msa": "76.12(+2.51)", "fsa": "69.70(+0.92)"}, "llama_3_1_2b_icldist": {"msa": "74.64(+1.03)", "fsa": "69.30(+0.52)"}}, "table_4_optimization_comparison_f1_score": {"unit": "%", "llama_3_1_2b": {"bsa": 82.54, "msa": 45.06, "fsa": 23.72}, "llama_3_1_2b_knowdist": {"bsa": "83.65(+1.01)", "msa": "50.65(+5.59)", "fsa": "27.11(+3.39)"}, "llama_3_1_2b_icldist": {"bsa": "87.83(+5.29)", "msa": "58.75(+13.69)", "fsa": "27.55(+3.83)"}, "llama_3_1_2b_unified": {"bsa": "87.21(+4.67)", "msa": "53.57(+8.51)", "fsa": "27.45(+3.73)"}, "llama_3_1_2b_two_stage": {"bsa": "88.06(+5.52)", "msa": "60.70(+15.64)", "fsa": "27.74(+4.02)"}}, "table_5_knowdist_ablation_f1_score": {"unit": "%", "baseline_no_dist": {"bsa": 82.54, "msa": 45.06, "fsa": 23.72}, "knowdist_analyzing_no_mpp": {"bsa": "83.69(+1.15)", "msa": "45.72(+0.66)", "fsa": "26.07(+2.35)"}, "knowdist_analyzing_with_mpp": {"bsa": "83.92(+1.38)", "msa": "49.62(+4.56)", "fsa": "27.53(+3.81)"}, "knowdist_rewriting_no_mpp": {"bsa": "83.44(+0.90)", "msa": "44.98(–0.08)", "fsa": "24.85(+1.13)"}, "knowdist_rewriting_with_mpp": {"bsa": "82.77(+0.23)", "msa": "47.90(+2.84)", "fsa": "26.02(+2.30)"}, "knowdist_analyzing_rewriting_with_mpp": {"bsa": "83.65(+1.11)", "msa": "50.65(+5.59)", "fsa": "27.11(+3.39)"}}, "table_6_icldist_ablation_f1_score": {"unit": "%", "baseline_no_dist": {"seen": 77.65, "unseen": 31.0}, "icldist_no_diversification": {"seen": "85.36(+7.71)", "unseen": "33.53(+2.53)"}, "icldist_lw_only": {"seen": "85.18(+7.53)", "unseen": "33.91(+2.91)"}, "icldist_lw_lt": {"seen": "85.44(+7.79)", "unseen": "34.07(+3.07)"}, "icldist_lw_lt_mi": {"seen": "85.08(+7.43)", "unseen": "35.09(+4.09)"}, "icldist_td_only": {"seen": "85.64(+7.99)", "unseen": "37.52(+6.52)"}, "icldist_all_diversification": {"seen": "85.01(+7.36)", "unseen": "38.79(+7.79)"}, "generalization_gain": "Diversification strategies improved unseen tasks performance from 2.53% to 7.79%."}, "table_7_teacher_llms_effect_f1_score": {"unit": "%", "no_distill": {"bsa": 82.54, "msa": 45.06, "fsa": 23.72}, "teacher_llama_3_1_2b": {"bsa": "80.45(-2.09)", "msa": "46.33(+1.27)", "fsa": "22.53(-1.19)"}, "teacher_llama_3_3_2b": {"bsa": "85.85(+3.31)", "msa": "51.05(+5.99)", "fsa": "27.59(+3.87)"}, "teacher_llama_3_8b": {"bsa": "85.90(+3.36)", "msa": "57.16(+12.10)", "fsa": "29.02(+5.30)"}, "teacher_llama_3_70b": {"bsa": "88.06(+5.52)", "msa": "60.70(+15.64)", "fsa": "27.74(+4.02)"}}, "table_8_mmlu_accuracy": {"unit": "%", "llama_3_1_2b_base": {"human": 42.87, "social": 51.16, "stem": 39.68, "other": 52.11, "avg": 46.12}, "llama_3_1_2b_know_icldist": {"human": 43.14, "social": 52.62, "stem": 40.17, "other": 53.4, "avg": 46.94}, "mmlu_impact": "Distillation resulted in a slight improvement (+0.82% avg) on MMLU, indicating no degradation of general abilities."}}}, "temporal_info": {"publication_date": "05 Mar 2025"}, "geographical_data": {"affiliations": ["Harbin Institute of Technology, Shenzhen, China", "Peng Cheng Laboratory, Shenzhen, China", "Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies", "The 30th Research Institute of China Electronics Technology Group Corporation"]}, "references": [{"type": "benchmark", "name": "SentiBench", "description": "Comprehensive sentiment analysis benchmark introduced in this paper."}, {"type": "dataset", "name": "IMDb", "citation": "Maas et al. (2011)"}, {"type": "dataset", "name": "Yelp2", "citation": "Zhang et al. (2015)"}, {"type": "dataset", "name": "SST2", "citation": "Socher et al. (2013)"}, {"type": "dataset", "name": "Twitter17", "citation": "Rosenthal et al. (2017)"}, {"type": "dataset", "name": "Irony18", "citation": "Van Hee et al. (2018)"}, {"type": "dataset", "name": "Emotion20", "citation": "Mohammad et al. (2018); Barbieri et al. (2020)"}, {"type": "dataset", "name": "P-Stance", "citation": "Li et al. (2021)"}, {"type": "dataset", "name": "Mint-English", "citation": "Pei et al. (2023)"}, {"type": "dataset", "name": "Rest16", "citation": "Pontiki et al. (2016); Zhang et al. (2021)"}, {"type": "dataset", "name": "Opener", "citation": "Barnes et al. (2022)"}, {"type": "dataset", "name": "Super-NaturalInstructions", "citation": "Wang et al. (2022)"}, {"type": "model", "name": "Llama-3.1-70B-Instruct", "citation": "Grattafiori et al. (2024)"}, {"type": "model", "name": "Llama-3.2-1.2B-Instruct", "citation": "Not explicitly cited, but part of Llama-3 family (Grattafiori et al., 2024)"}, {"type": "model", "name": "T5-base", "citation": "Raffel et al. (2020)"}, {"type": "model", "name": "OPT-1.3B", "citation": "Zhang et al. (2022)"}, {"type": "model", "name": "TinyLlama-1.1B-Chat-v1.0", "citation": "Zhang et al. (2024a)"}, {"type": "model", "name": "Phi-2-2.7B", "citation": "Microsoft (implied)"}, {"type": "model", "name": "Qwen-2.5-1.5B-Instruct", "citation": "Alibaba Cloud (implied)"}, {"type": "model", "name": "Gemma-2-2.6B-it", "citation": "Gemma Team (2024)"}, {"type": "model", "name": "GPT-3.5", "citation": "OpenAI (implied)"}, {"type": "dataset", "name": "Alpaca-data", "citation": "Taori et al. (2023)"}, {"type": "dataset", "name": "Lamini-data", "citation": "Wu et al. (2024)"}, {"type": "benchmark", "name": "MMLU (Massive Multitask Language Understanding)", "citation": "Hendrycks et al. (2021)"}]}