{"extracted_information": "WebLLM is a high-performance in-browser LLM inference engine that enables language model inference directly within web browsers with hardware acceleration. It runs entirely client-side without server support and is accelerated by WebGPU. It is fully compatible with OpenAI API for functionalities like streaming, JSON-mode, logit-level control, and seeding. It is developed as a companion project to MLC LLM, allowing universal deployment of LLMs across hardware environments.", "specifications": {"framework_type": "In-browser LLM inference engine", "core_technology": "WebGPU for hardware acceleration, WebAssembly for model library execution", "programming_language": "TypeScript (components rewritten)", "package_management": "npm, yarn, pnpm (npm package: @mlc-ai/web-llm), CDN delivery (jsdelivr.com, esm.run)", "dependencies": {"tvmjs_version_hint": "0.18.0-dev0 (npm package @mlc-ai/tvmjs)", "web_tokenizers_version": "0.1.6", "tvmjs_from_source_requirements": {"compiler": "Emscripten", "emcc_version_recommendation": "3.1.56 (to avoid LinkError: WebAssembly.instantiate() issues)", "tvm_source": "mlc-ai/relax (HEAD) or specific commit from apache/tvm, requires --recursive clone"}}, "performance_benchmarks": {"llama2_7b_q4f32_1_on_m3_max": {"prompt_tokens": 12, "decoded_tokens": 128, "prefill_speed": "182 tok/s", "decode_speed": "48.3 tok/s", "end_to_end_speed": "38.5 tok/s"}}}, "pricing": {}, "features": [{"name": "In-Browser Inference", "description": "High-performance, in-browser language model inference engine leveraging WebGPU for hardware acceleration, operating directly within web browsers without server-side processing."}, {"name": "Full OpenAI API Compatibility", "description": "Seamless integration via OpenAI API with functionalities such as streaming, JSON-mode, logit-level control, seeding, and function-calling (WIP)."}, {"name": "Structured JSON Generation", "description": "Supports state-of-the-art JSON mode structured generation, implemented in the WebAssembly portion of the model library for optimal performance."}, {"name": "Extensive Model Support", "description": "Natively supports various models including Llama 3, Phi 3, Gemma, Mistral, Qwen (通义千问), and others. Integrates with MLC Models for a broader list."}, {"name": "Custom Model Integration", "description": "Allows integration and deployment of custom models in MLC format by specifying model artifacts and WebAssembly library URLs."}, {"name": "Plug-and-Play Integration", "description": "Easy integration via NPM, Yarn, or CDN with comprehensive examples and a modular design."}, {"name": "Streaming & Real-Time Interactions", "description": "Supports streaming chat completions for real-time output generation, enhancing interactive applications."}, {"name": "Web Worker & Service Worker Support", "description": "Optimizes UI performance and model lifecycle by offloading computations to separate worker threads or service workers. Service workers can enable offline experience and avoid model reloading across page visits."}, {"name": "Chrome Extension Support", "description": "Enables building custom Chrome extensions, including basic and WebGPU service worker based persistent extensions."}], "statistics": {}, "temporal_info": {"latest_version_bump": "0.2.79 (May 5, 2025)", "major_overhaul_date": "May 25, 2023"}, "geographical_data": {}, "references": [{"type": "Documentation", "url": "https://webllm.mlc.ai/docs/"}, {"type": "Blogpost", "url": "https://blog.mlc.ai/2024/06/13/webllm-a-high-performance-in-browser-llm-inference-engine"}, {"type": "Paper", "url": "https://arxiv.org/abs/2412.15803", "citation": "@misc{ruan2024webllmhighperformanceinbrowserllm,\ntitle={WebLLM: A High-Performance In-Browser LLM Inference Engine},\nauthor={Charlie F. Ruan and Yucheng Qin and Xun Zhou and Ruihang Lai and Hongyi Jin and Yixin Dong and Bohan Hou and Meng-Shiun Yu and Yiyan Zhai and Sudeep Agarwal and Hangrui Cao and Siyuan Feng and Tianqi Chen},\nyear={2024},\neprint={2412.15803},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2412.15803},\n}"}, {"type": "Demo App", "url": "https://chat.webllm.ai/"}, {"type": "Related Project", "name": "MLC LLM", "url": "https://github.com/mlc-ai/mlc-llm"}, {"type": "Related Project", "name": "Web Stable Diffusion", "url": "https://github.com/mlc-ai/web-stable-diffusion/"}, {"type": "Model List", "url": "https://mlc.ai/models"}, {"type": "Prebuilt App Config Model List", "url": "https://github.com/mlc-ai/web-llm/blob/main/src/config.ts#L293"}], "supported_models": {"primary_families": ["Llama", "Phi", "Gemma", "Mistral", "Qwen (通义千问)"], "specific_models_mentioned": ["Llama 3", "Llama 2", "Hermes-2-Pro-Llama-3", "Phi 3", "Phi 2", "Phi 1.5", "Gemma-2B", "Mistral-7B-v0.3", "Hermes-2-Pro-Mistral-7B", "NeuralHermes-2.5-Mistral-7B", "OpenHermes-2.5-Mistral-7B", "Qwen2 0.5B", "Qwen2 1.5B", "Qwen2 7B", "Qwen3-0.6B (q0f16, q0f32, q4f16_1, q4f32_1)", "Qwen3 {1.7B, 4B, 8B} (q4f16_1, q4f32_1)"], "custom_models_support": "Yes, in MLC format, by providing URLs to model artifacts and the WebAssembly library (wasm file).", "model_quantization_formats": ["q0f16", "q0f32", "q4f16_1", "q4f32_1"]}, "system_requirements": {"hardware_acceleration": "Requires hardware acceleration, primarily WebGPU.", "browser_compatibility": "Supports modern web browsers with WebGPU enabled.", "runtime_environment": "Runs within web browsers. For development, a Node.js environment (for npm) is implied.", "build_from_source_requirements": {"emscripten": "Required for building TVMjs from source.", "emcc_version_recommendation": "Use emcc version 3.1.56 as a workaround to avoid runtime issues (LinkError: WebAssembly.instantiate())."}}, "webgpu_compatibility": "WebLLM is accelerated by WebGPU. It leverages WebGPU for high-performance in-browser inference. Unit tests and Chrome extension examples also demonstrate WebGPU usage.", "deployment_constraints_and_methods": {"deployment_methods": ["NPM package integration", "Yarn package integration", "PNPM package integration", "CDN delivery", "Custom web application build", "Chrome Extension"], "in_browser_execution": "All operations run directly within the web browser.", "no_server_support_required": "No backend server is needed for inference, ensuring privacy and local execution.", "worker_usage_recommendation": "Recommended to use Web Workers or Service Workers to offload heavy computations, preventing UI disruption.", "service_worker_specifics": {"lifecycle_management": "Managed by the browser; can be killed without notification.", "stability_considerations": "Requires proper error handling in the application. `ServiceWorkerMLCEngine` attempts to keep the worker alive via heartbeat events (`keepAliveMs`, `missedHeartbeat`).", "benefits": "Avoids model reloading on page visits, enables offline capabilities."}, "model_loading": "Initial model loading can be time-consuming due to downloading models; proper asynchronous handling and caching are important.", "model_artifacts": "Custom models require accessible URLs for model weights and WebAssembly libraries."}}