{"extracted_information": "Phi-3.5-mini-instruct is a lightweight, state-of-the-art open model from the Phi-3 family, built on high-quality, reasoning-dense synthetic and filtered public data. It supports a 128K token context length and underwent supervised fine-tuning, proximal policy optimization, and direct preference optimization for precise instruction adherence and safety. The model is intended for commercial and research use, especially in memory/compute constrained or latency-bound environments, and for tasks requiring strong reasoning in code, math, and logic. It functions as a building block for generative AI applications and research on language/multimodal models.", "specifications": {"model_name": "Phi-3.5-mini-instruct", "architecture": "Dense decoder-only Transformer", "parameters": "3.8B", "context_length": "128K tokens", "tokenizer_vocabulary_size": "up to 32064 tokens", "inputs": "Text, best suited for chat format prompts", "outputs": "Generated text in response to input", "supported_languages": ["Arabic", "Chinese", "Czech", "Danish", "Dutch", "English", "Finnish", "French", "German", "Hebrew", "Hungarian", "Italian", "Japanese", "Korean", "Norwegian", "Polish", "Portuguese", "Russian", "Spanish", "Swedish", "Thai", "Turkish", "Ukrainian"], "training_data_size": "3.4T tokens", "training_gpus": "512 H100-80G", "training_time": "10 days", "training_data_cutoff_date": "October 2023 (for publicly available data)", "release_date": "August 2024", "recommended_transformers_version": "4.43.0", "required_packages": ["flash_attn==2.5.8", "torch==2.3.1", "accelerate==0.31.0", "transformers==4.43.0"], "gpu_compatibility": {"flash_attention_supported": ["NVIDIA A100", "NVIDIA A6000", "NVIDIA H100"], "eager_attention_required": ["NVIDIA V100", "earlier generation GPUs"]}, "license": "MIT license"}, "pricing": {}, "features": ["Lightweight model", "State-of-the-art open model", "Precise instruction adherence (via SFT, PPO, DPO)", "Robust safety measures", "Multilingual capabilities (improved multi-turn conversation quality and reasoning)", "Strong reasoning (code, math, logic)", "Long context understanding (summarization, QA, information retrieval for documents up to 128K tokens)", "Optimized for memory/compute constrained environments", "Optimized for latency bound scenarios", "Building block for generative AI powered features"], "statistics": {"performance_benchmarks": {"overall_average": "61.4", "multilingual_benchmarks_average": "55.2", "multilingual_mmlu": {"phi-3_5_mini-ins": 55.4, "phi-3_0_mini-128k-instruct_(june2024)": 51.08, "mistral-7b-instruct-v0_3": 47.4, "mistral-nemo-12b-ins-2407": 58.9, "llama-3_1-8b-ins": 56.2, "gemma-2-9b-ins": 63.8, "gemini_1_5_flash": 77.2, "gpt-4o-mini-2024-07-18_(chat)": 72.9}, "multilingual_mmlu-pro": {"phi-3_5_mini-ins": 30.9, "phi-3_0_mini-128k-instruct_(june2024)": 30.21, "mistral-7b-instruct-v0_3": 15, "mistral-nemo-12b-ins-2407": 34, "llama-3_1-8b-ins": 21.4, "gemma-2-9b-ins": 43, "gemini_1_5_flash": 57.9, "gpt-4o-mini-2024-07-18_(chat)": 53.2}, "mgsm": {"phi-3_5_mini-ins": 47.9, "phi-3_0_mini-128k-instruct_(june2024)": 41.56, "mistral-7b-instruct-v0_3": 31.8, "mistral-nemo-12b-ins-2407": 63.3, "llama-3_1-8b-ins": 56.7, "gemma-2-9b-ins": 75.1, "gemini_1_5_flash": 75.8, "gpt-4o-mini-2024-07-18_(chat)": 81.7}, "mega_mlqa": {"phi-3_5_mini-ins": 61.7, "phi-3_0_mini-128k-instruct_(june2024)": 55.5, "mistral-7b-instruct-v0_3": 43.9, "mistral-nemo-12b-ins-2407": 61.2, "llama-3_1-8b-ins": 45.2, "gemma-2-9b-ins": 54.4, "gemini_1_5_flash": 61.6, "gpt-4o-mini-2024-07-18_(chat)": 70}, "mega_tydi_qa": {"phi-3_5_mini-ins": 62.2, "phi-3_0_mini-128k-instruct_(june2024)": 55.9, "mistral-7b-instruct-v0_3": 54, "mistral-nemo-12b-ins-2407": 63.7, "llama-3_1-8b-ins": 54.5, "gemma-2-9b-ins": 65.6, "gemini_1_5_flash": 63.6, "gpt-4o-mini-2024-07-18_(chat)": 81.8}, "mega_udpos": {"phi-3_5_mini-ins": 46.5, "phi-3_0_mini-128k-instruct_(june2024)": 48.1, "mistral-7b-instruct-v0_3": 57.2, "mistral-nemo-12b-ins-2407": 58.2, "llama-3_1-8b-ins": 54.1, "gemma-2-9b-ins": 56.6, "gemini_1_5_flash": 62.4, "gpt-4o-mini-2024-07-18_(chat)": 66}, "mega_xcopa": {"phi-3_5_mini-ins": 63.1, "phi-3_0_mini-128k-instruct_(june2024)": 62.4, "mistral-7b-instruct-v0_3": 58.8, "mistral-nemo-12b-ins-2407": 10.8, "llama-3_1-8b-ins": 21.1, "gemma-2-9b-ins": 31.2, "gemini_1_5_flash": 95, "gpt-4o-mini-2024-07-18_(chat)": 90.3}, "mega_xstorycloze": {"phi-3_5_mini-ins": 73.5, "phi-3_0_mini-128k-instruct_(june2024)": 73.6, "mistral-7b-instruct-v0_3": 75.5, "mistral-nemo-12b-ins-2407": 92.3, "llama-3_1-8b-ins": 71, "gemma-2-9b-ins": 87, "gemini_1_5_flash": 20.7, "gpt-4o-mini-2024-07-18_(chat)": 96.6}, "long_context_average": "26.1", "ruler_benchmark_average": "84.1", "repoqa_benchmark_average": "77", "reasoning_category_score": "70.1", "language_understanding_category_score": "62.6", "multilingual_category_score": "55.2", "downloads_last_month": 231258}, "limitations": ["Performance disparities for languages other than English, and less represented English varieties.", "Challenges in multilingual performance and safety across different languages.", "Potential for over/under-representation, erasure of groups, or reinforcement of stereotypes due to training data bias.", "May produce inappropriate or offensive content.", "Can generate nonsensical, inaccurate, or outdated information (hallucinations).", "Limited scope for code generation outside of Python and common packages (typing, math, random, collections, datetime, itertools).", "May produce repetitive, unhelpful, or inconsistent responses in very long chat sessions (conversational drift).", "Limited capacity to store vast factual knowledge, leading to factual incorrectness (can be mitigated with RAG).", "Models may be more susceptible to longer multi-turn jailbreak techniques across both English and non-English languages."]}, "temporal_info": {"training_period": "June to August 2024", "data_cutoff_date": "October 2023 (for publicly available data)", "release_date": "August 2024", "previous_release": "June 2024 instruction-tuned Phi-3 Mini"}, "geographical_data": {}, "references": ["https://huggingface.co/microsoft/Phi-3.5-mini-instruct", "https://azure.microsoft.com/en-us/products/phi-3 (Phi-3 Portal)", "https://aka.ms/phi3.5-techblog (Phi-3 Microsoft Blog)", "https://arxiv.org/abs/2404.14219 (Phi-3 Technical Report)", "https://github.com/microsoft/Phi-3CookBook (Phi-3 Cookbook)", "https://aka.ms/try-phi3.5mini (Try It/Azure AI Studio)", "https://huggingface.co/microsoft/Phi-3.5-mini-instruct/blob/main/added_tokens.json (Tokenizer files)", "https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/sample_finetune.py (Sample finetune code)", "https://arxiv.org/pdf/2407.13833 (Phi-3 Safety Post-Training paper)", "https://arxiv.org/abs/2403.06412 (CLIcK paper for Korean benchmarks)"]}