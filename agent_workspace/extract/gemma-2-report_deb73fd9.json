{"origin_pdf_path": "https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf", "text_in_pdf": "Gemma 2: Improving Open Language Models at a Practical Size  \n\nGemma Team, Google DeepMind1  \n\nIn this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are $\\mathbf{2-3\\times}$ bigger. We release all our models to the community.  \n\n1. Introduction  \n\nLarge language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning (Brown et al., 2020; Radford et al., 2019; Raffel et al., 2019). Scaling has been key to this recent progress, with many new capabilities only emerging at scale (Brown et al., 2020). The newest large models not only reach unprecedented performance on reasoning benchmarks (Achiam et al., 2023), but they also demonstrate multimodal and multilingual capabilities (Gemini Team, 2024) and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024).  \n\nSmall-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than $1\\!-\\!2\\%$ (AI@Meta, 2024).  \n\nYet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective.  \n\nIn particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than $50\\times$ the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work.  \n\nWe also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023).  \n\nOverall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., 2019; Suzgun et al., 2022), mathematics and science (Cobbe et al., 2021; Hendrycks et al., 2020), and coding (Austin et al., 2021; Chen et al., 2021).  \n\nParameters2B9B27Bd_model230435844608Layers264246Pre-norm Post-normyesyesyesNon-linearityyes GeGLUyes GeGLUyes GeGLUFeedforward dim Head type184322867273728Num headsGQA 8GQA 16GQA 32Num KV heads4Head size816256256128Global att. span Sliding window8192 409681928192Vocab size40964096256128256128256128Tied embeddingyesyesyes\n\nTable 1 | Overview of the main model parameters and design choices. See the section on model architectures for more details.  \n\nWhile thorough testing of our models has been conducted, these tests cannot cover all applications and scenarios in which Gemma 2 may be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use.  \n\nIn this technical report, we provide an overview of models, including the architecture, training, and pre- and post-training recipes for Gemma 2. We also provide detailed evaluations across a wide variety of quantitative and qualitative benchmarks, as well as both standard academic benchmarks and human-preference evaluations. Finally, we discuss our approach to safe and responsible deployment and outline the broader implications of Gemma 2, its limitations, and advantages.  \n\n2. Model Architecture  \n\nSimilar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017). We summarize the main parameters and architecture choices in Table 1.  \n\nA few architectural elements are similar to the first version of Gemma models; namely, a context  \n\nModelEmbedding ParametersNon-embedding Parameters2B590,118,9122,024,517,8889B917,962,7528,324,201,98427B1,180,237,82426,047,480,320  \n\nTable 2 | Parameter counts for the Gemma models. We inherit from the large Gemini vocabulary (256k entries), that is designed to work on a large number of languages, hence, the larger embedding parameter counts compared to models that are limited to one or a few languages.  \n\nlength of 8192 tokens, the use of Rotary Position Embeddings (RoPE) (Su et al., 2021), and the approximated GeGLU non-linearity (Shazeer, 2020). A few elements differ between Gemma 1 and Gemma 2, including using deeper networks. We summarize the key differences below.  \n\nLocal Sliding Window and Global Attention. We alternate between a local sliding window attention (Beltagy et al., 2020a,b) and global attention (Luong et al., 2015) in every other layer. The sliding window size of local attention layers is set to 4096 tokens, while the span of the global attention layers is set to 8192 tokens.  \n\nLogit soft-capping. We cap logits (Bello et al., 2016) in each attention layer and the final layer such that the value of the logits stays between −soft_cap and +soft_cap. More specifically, we cap the logits with the following function:  \n\n$$\n\\mathrm{logits}\\gets\\mathrm{soft\\_cap}\\mathrm{tanh}(\\mathrm{logits}/\\mathrm{soft\\_cap}).\n$$  \n\nWe set the soft_cap parameter to 50.0 for the selfattention layers and to 30.0 for the final layer.  \n\nPost-norm and pre-norm with RMSNorm. To stabilize training, we use RMSNorm (Zhang and Sennrich, 2019) to normalize the input and output of each transformer sub-layer, the attention layer, and the feedforward layer.  \n\nGrouped-Query Attention (Ainslie et al., 2023). We use GQA with num_groups $=2$ , based on ablations showing increased speed at inference time while maintaining downstream performance.  \n\n3. Pre-training  \n\nWe provide a brief overview of the parts of our pre-training that differs from Gemma 1.  \n\n3.1. Training Data  \n\nWe train Gemma 2 27B on 13 trillion tokens of primarily-English data, the 9B model on 8 trillion tokens, and the 2B on 2 trillion tokens. These tokens come from a variety of data sources, including web documents, code, and science articles. Our models are not multimodal and are not trained specifically for state-of-the-art multilingual capabilities. The final data mixture was determined through ablations similar to the approach in Gemini 1.0 (Gemini Team, 2023).  \n\nTokenizer. We use the same tokenizer as Gemma 1 and Gemini: a SentencePiece tokenizer with split digits, preserved whitespace, and byte-level encodings (Kudo and Richardson, 2018). The resulting vocabulary has 256k entries.  \n\nFiltering. We use the same data filtering techniques as Gemma 1. Specifically, we filter the pretraining dataset to reduce the risk of unwanted or unsafe utterances, filter out certain personal information or other sensitive data, decontaminate evaluation sets from our pre-training data mixture, and reduce the risk of recitation by minimizing the proliferation of sensitive outputs.  \n\nShardsModelType#ChipsDataModel2BTPUv5e51251219BTPUv440961024427BTPUv5p61447688\n\nTable 3 | Training infrastructure with sharding.  \n\n3.2. Knowledge Distillation  \n\nGiven a large model used as a teacher, we learn smaller models by distilling from the probability given by the teacher of each token $x$ given its context $x_{c}$ , i.e., $P_{T}(x\\mid x_{c})$ . More precisely, we minimize the negative log-likelihood between the  \n\nContextRelevantTokenUserturnuserModelturnmodelStartofconversationturnEndofconversationturnBeginning of sequenceEndofsequence  \n\nTable 4 | Relevant formatting control tokens used for Gemma models.  \n\nprobabilities from the teacher and the student:  \n\n$$\n\\operatorname{min}_{P_{S}}\\sum_{x}-P_{T}(x\\mid x_{c})\\log P_{S}(x\\mid x_{c}),\n$$  \n\nwhere $P_{S}$ is the parameterized probability of the student. Note that knowledge distillation was also used in Gemini 1.5 (Gemini Team, 2024).  \n\n3.3. Compute Infrastructure  \n\nWe train our models with TPUv4, TPUv5e, and TPUv5p as outlined in Table 3. For the 2B model, we train on a 2x16x16 configuration of TPUv5e, totaling 512 chips, with 512-way data replication and 1-way model sharding. For the 9B model, we train on an $8\\mathrm{x}16\\mathrm{x}32$ configuration of TPUv4, totaling 4096 chips, with 1024-way data replication and 4-way model sharding. For the 27B model, we train on an $8\\mathbf{x}24\\mathbf{x}32$ configuration of TPUv5p, totaling 6144 chips, with 768-way data replication and 8-way model sharding.  \n\nThe optimizer state is further sharded using techniques similar to ZeRO-3 (Ren et al., 2021). For scales beyond a single pod, we perform a data-replica reduction over the data center network, using the Pathways approach of Barham et al. (2022). We also use the ’single controller’ programming paradigm of Jax (Roberts et al., 2023) and Pathways (Barham et al., 2022). As in Gemma 1, we use the GSPMD partitioner (Xu et al., 2021) for training step computation and the MegaScale XLA compiler (XLA, 2019).  \n\n3.4. Carbon Footprint  \n\nWe estimate the carbon emissions from pretraining the Gemma models to be $1247.61\\,t C O_{2}e q$ . As in Gemma 1 (Gemma Team, 2024), this value is calculated based on the hourly energy usage reported directly from our TPU data centers and scaled to account for the additional energy expended to create and maintain the data center. Importantly, Google data centers are carbon neutral, achieved through a combination of energy efficiency, renewable energy purchases, and carbon offsets. This carbon neutrality applies to our experiments and the machines running them.  \n\n4. Post-Training  \n\nFor post-training, we fine-tune our pre-trained models into instruction-tuned models. First, we apply supervised fine-tuning (SFT) on a mix of text-only, English-only synthetic and humangenerated prompt-response pairs. We then apply RLHF on top of these models with the reward model trained on labelled English-only preference data and the policy based on the same prompts as the SFT phase. Finally, we average the models obtained after each phase to improve their overall performance. The final data mixtures and post-training recipe, which includes tuned hyperparameters, were chosen on the basis of improving helpfulness while minimizing model harms related to safety and hallucinations.  \n\nWe extended the post-training data from Gemma 1.1 with a mixture of internal and external public data. In particular, we use the prompts, but not the answers from LMSYS-chat-1M (Zheng et al., 2023). All of our data go through a filtering stage described below.  \n\nSupervised fine-tuning (SFT). We run behavioral cloning on synthetic and real prompts, and responses predominantly synthetically generated by the teacher, that is a larger model. We also run distillation from the teacher on the student’s distribution (Agarwal et al., 2024; Gu et al., 2024).  \n\nReinforcement Learning from Human Feedback (RLHF). We use a similar RLHF algorithm as Gemma 1.1 (Gemma Team, 2024) but a different reward model, which is an order of magnitude  \n\nFirst turnUser:userKnock knock. modelModel:Who'sthere?Second turnUser:userKnock knock.Model:modelUser:Who'sthere?userGemma.modelModel:Gemma who?  \n\nTable 5 | Example dialogue with user and model control tokens. To proceed with multi-turn, remove the model-outputted , add back the usual user turn’s control tokens and continue with the following turn’s chat template.  \n\nlarger than the policy. The new reward model is also oriented more towards conversational capabilities, specifically multi-turn.  \n\nModel merging. We average different models obtained by running our pipeline with different hyperparameters (Ramé et al., 2024).  \n\nData filtering. When using synthetic data, we run several stages of filtering to remove examples that show certain personal information, unsafe or toxic model outputs, mistaken self-identification data, and duplicated examples. Following Gemini, we find that including subsets of data that encourage better in-context attribution, hedging, and refusals to minimize hallucinations improves performance on factuality metrics, without degrading model performance on other metrics.  \n\nFormatting. Gemma 2 models are fine-tuned with the same control tokens as Gemma 1 models, as detailed in Table 4, but a different formatting schema. See the dialogue example in Table 5. Notice that the model explicitly ends generations with  tokens, while previously it only generated . For the motivation behind this formatting structure, see Gemma 1.  \n\n5. Ablations  \n\nIn this section, we focus on the main finding of this work, which is the impact of knowledge distillation on small language models.  \n\nfrom scratchdistilledAverage (3 bench.)60.367.7  \n\nTable 6 | Comparison between a 2B model trained over 500B tokens either from scratch or with distillation from a 7B model.  \n\nDistillation versus from scratch. In Table 6, we show that distilling from a larger model improves performance compared to training from scratch. Note that 500B is $10\\times$ more than the computeoptimal number of tokens for a 2B model. We distill from a 7B model to keep a ratio similar to our target distillation from 27B to 9B.  \n\n200M400M1Bfrom scratch231917distilled (7B)211715  \n\nTable 7 | Perplexity measured on a validation set of models of different sizes trained with or without distillation. The teacher has 7B parameters.  \n\nImpact of distillation w.r.t. model size. In Table 7, we measure the impact of distillation as model size increases. We observe that the gain remains as the model size is scaled. In this ablation, we maintain the size of the teacher at 7B and train smaller models to simulate the same gap as between our final teacher and student sizes.  \n\nMHAGQAAverage (4 bench.)50.350.8  \n\nTable 8 | Comparing the impact of replacing MultiHead Attention (MHA) with GQA on a 9B model averaged over 4 benchmarks.  \n\nGQA versus MHA. In Table 8, we compare two instances of our 9B with MHA or GQA. We observe overall few changes in performance between both models as measured on several benchmarks. We choose GQA since it requires fewer parameters and is faster at inference time.  \n\nWide versus deep. In Table 9, we show that a deeper 9B network is slightly better than a wider 9B for the same number of parameters. Although the gap is small, it is consistent across benchmarks and warrants the switch to a deeper architecture.  \n\nWideDeepAverage (4 bench.)50.852.0  \n\nTable 9 | Wide versus deep 9B models. Performance on 4 benchmarks, higher is better.  \n\nChanging sliding window size. In Table 10, we show that we can change the sliding window size of the local attention layers of the models during inference with moderate impact on perplexity. Adjusting the size of the sliding window can thus be a leverage for slight inference speed gain.  \n\nsliding window409620481024perplexity (val. set)1.631.631.64  \n\nTable 10 | Impact of changing the sliding window size at inference time for the 9B model.  \n\nImpact of formatting. We measure performance variance on MMLU across prompt/evaluation formatting variations. Table 11 shows the standard deviations of MMLU scores for 12 formatting/evaluation combinations, a proxy for undesired performance variability. The Gemma 2B models are slightly less format-robust than the larger ones. Notably, Mistral 7B is significantly less robust than our models.  \n\nStandard DeviationGemma 1 2B 1.5 Gemma22B 2.1Mistral 7B 6.9Gemma17B 0.7Gemma29B 0.9Gemma227B 1.0  \n\nTable 11 | Standard deviations of MMLU scores for 12 combinations of formatting and evaluation.  \n\n6. Evaluation  \n\nIn this section, we evaluate both pre-trained and IT models over a series of automated benchmarks and human evaluations across a variety of domains. We also report performance from models of similar sizes that have permissive licenses, or as reported by others. Note that we consider total parameters, not active parameters, since total memory usage is often what limits the use of open models on standard devices.  \n\n6.1. Pre-training Evaluations  \n\nEvaluating the 27B model  \n\nIn this set of evaluations, we evaluate the performance of our 27B model trained without distillation on 13T tokens. We report results in Table 12, where we compare with a model of similar size, Qwen1.5 34B (Team, 2024), and a model $2.5\\times$ larger, LLaMA-3 70B on the HuggingFace evaluation suite. We selected these models based on their ranking on the HuggingFace leaderboard.  \n\nOverall, we observe that our model is the best in its size category and is even competitive with a larger model that is trained for longer. That being said, the performance of models trained in a similar fashion improves only logarithmically with their size and hence, our model is likely in the same Pareto curve as the LLaMA-3 models. However, it is not clear how these differences affect the quality of the resulting IT models.  \n\nEvaluating the 2B and 9B models  \n\nIn this set of experiments, we compare our new 2B and 9B trained with distillation to our previous models and several standard open models in Gemma Team (2024).  \n\nWe observe overall a massive improvement in our models compared to previous versions, by up to $10\\%$ in some benchmarks for the 9B model. The two 2B models were trained with a similar number of tokens (2T for Gemma 2 and 3T for Gemma 1) and we still observe a significant improvement for the new models. This confirms that distillation significantly improves the quality of models even when trained on the same number of tokens.  \n\nLLaMA-3 70BQwen1.5 32BGemma-2 27BMMLU79.274.375.2GSM8K76.961.174.0ARC-C68.863.671.4HellaSwag88.085.086.4Winogrande85.381.583.7  \n\nTable 12 | We compare, on the HuggingFace benchmark, our 27B model with a competitive open model, Qwen1.5 32B, that has a similar size. We also report the performance of LLaMA-3 70B for completeness. Note that our model outperforms Qwen1.5 32B and is only a few percent below LLaMA-3 70B despite being $2.5\\times$ smaller and trained on 2/3rds less data.  \n\n6.2. Post-training Evaluations  \n\nIn this section, we evaluate our IT models on a set of human evaluations as well as standard academic benchmarks. The Gemma 2 models push the frontier for post-trained open-weights models, setting a new state of the art on the LMSYS Chatbot Arena (Chiang et al., 2024).  \n\nLMSYS Chatbot Arena  \n\nGemma 2 Instruction Tuned models were evaluated on the Chatbot Arena (Chiang et al., 2024) in blind side by side evaluations by human raters against other state of the art models. We report Elo scores in Table 14. Gemma 2.6B, 9B and 27B strongly outperform all other open models in the same range of parameters, with notably: Gemma 27B (Elo 1218) ranked higher than Llama 3 70B (Elo 1206), Gemma 9B (Elo 1187) similar as GPT-4-0314 (Elo 1186), Gemma 2.6B (Elo 1126) ranked higher than GPT-3.5-Turbo0613 (Elo 1116).  \n\nHuman Preference Evaluations  \n\nWe also submit Gemma IT models for side-byside human evaluation studies (which are independent from the Chatbot Arena). We used held-out collections of single-turn prompts that target safety and instruction following (IF). We use gpt4o-2024-05-13 as the base model, and  \n\nGemma-1 2BGemma-2 2BMistral 7BLLaMA-3Gemma-1Gemma-2Gemma-2Benchmarkmetric52.262.58B 66.67B9B 71.327BMMLU5-shot42.3 48.555.760.559.264.4 61.168.475.2 71.4ARC-C GSM8K25-shot 5-shot15.124.339.645.751.868.674.0AGIEval3-5-shot24.231.544.0t45.9t44.9t52.855.1DROP3-shot, F148.551.263.858.456.369.474.2BBH3-shot, CoT35.241.956.0°61.1°59.0°68.274.9Winogrande5-shot66.871.378.576.179.080.683.7HellaSwag10-shot71.772.983.082.082.381.986.4MATH4-shot11.816.012.724.336.642.3ARC-eO-shot73.280.680.581.588.088.6PIQAO-shot77.378.482.281.281.783.2SIQAO-shot49.751.947.051.853.453.7BoolqO-shot69.472.783.283.284.284.8TriviaQA5-shot53.260.462.563.476.683.7NQ5-shot12.517.123.223.029.234.5HumanEvalpass@122.020.126.232.340.251.8MBPP3-shot29.230.240.244.452.462.6Average (8)44.050.061.061.962.470.274.4Average (all)44.248.755.657.964.969.4  \n\nTable 13 | Comparison of models in the range of 2B to 9B parameters, as well as our 27B model, on a variety of benchmarks. We report the average performance on the 8 benchmarks where we can compare with LLaMA-3, and on all the benchmarks (all). The numbers for LLaMA-3 8B are either from the HuggingFace leaderboard or their blogpost. † we report the evaluation used in LLaMA-3 for the baselines, it leads to $+3\\%$ compared to our evaluation: Gemma-1 7B achieves $44.9\\%$ instead of $41.7\\%$ , and Mistral 7B, $44\\%$ instead of $41.2\\%$ . ⋄we report the evaluation used in LLaMA-3 for the baselines, it leads to $+4\\%$ compared to our evaluation for Gemma-1 7B, i.e., $59.0\\%$ instead of $55.1\\%$ . ∗these are evaluations run by us for Gemma 1 (Gemma Team, 2024).  \n\nobserve large improvements in win rates and preference scores as compared against the older Gemma 1.1 7B model. We report safety as a win-loss ratio against GPT4o, and we report single-sided instruction following scores as ratio of prompts where all instructions are followed. In particular, we find that regardless of their size, Gemma 2 models produce safer, more appropriate prompts on the held-out safety prompt set than GPT4o.  \n\nHuman Multi-Turn Evaluations  \n\nWe evaluated the multi-turn capabilities of Gemma 1.1 7B, Gemma 2 2B, 9B and 27B models by tasking human raters to have conversations with the models and follow specified given scenarios. We used a diverse, held-out set of 500 scenarios, each describing a sequence of requests to the model, including measuring instances of brainstorming, making a plan, or learning something new. The average number of user turns is 8.4. We found that the conversations with Gemma 2 models are rated significantly better than Gemma 1.1 in user satisfaction and conversation goal achievement (Table 16). Moreover, we saw that the Gemma 2 models were better than Gemma 1.1 7B at maintaining high quality of responses for the entire conversation.  \n\nStandard Benchmarks  \n\nIt has been observed in Llama-3 (AI $@$ Meta, 2024) that instruction fine-tuning can improve the performance of the models on few-shot benchmarks  \n\nModelElo95% CIOpenModelElo95% CIOpengpt-40-2024-05-131286+2/-3gemma-2-9b-it1187+3 / -5+gpt-40-mini-2024-07-181279+5 / -4qwen2-72b-instruct1187+3/-3+claude-3-5-sonnet1271+3 / -4-gpt-4-03141186+2/-3-gemini-advanced-05141266+2/-3-qwen1.5-110b-chat1161+3 /-3+llama-3.1-405b-instruct1262+8 / -7+mistral-large-24021157+3/ -3gemini-1.5-pro-api-05141261+2 / -3yi-1.5-34b-chat1157+4 / -3gemini-1.5-pro-api-04091257+3/-3reka-flash-21b-202402261155+4 / -4gpt-4-turbo-2024-04-091256+2/-3llama-3-8b-instruct1151+2/-3+gpt-4-1106-preview1250+3/-3command-r1148+3/-3+claude-3-0pus-202402291248+2 / -2claude-11148+4 / -4-athene-70b-07251245+8 / -6+mistral-medium1147+4 / -4gpt-4-0125-preview1245+2/ -2reka-flash-21b-202402261147+3 / -4llama-3.1-70b-instruct1244+8 / -9+qwen1.5-72b-chat1147+4 / -4+yi-large-preview1239+3/-3-mixtral-8x22b-instruct-v0.11145+2/-3+gemini-1.5-flash-api-05141227+3 /-3claude-2.01131+4 / -6deepseek-v2-api-06281220+6 / -6+gemini-pro-dev-api1131+4 / -3-gemma-2-27b-it1218+4 / -3+zephyr-orpo-141b1127+10 / -6+yi-large1212+4 / -5gemma-2-2b-it1126+10/ -10+nemotron-4-340b-instruct1209+3 / -4+qwen1.5-32b-chat1125+3/-3+bard-jan-24-gemini-pro1208+5 / -7mistral-next1124+5 / -5glm-4-05201206+3 / -5-phi-3-medium-4k-instruct1122+4 / -4+llama-3-70b-instruct1206+2/ -2+starling-lm-7b-beta1118+4 / -5+claude-3-sonnet1200+2 / -2claude-2.11118+3 / -3reka-c0re-202405011199+3/ -3-gpt-3.5-turbo-06131116+3 / -4command-r-plus1189+2 / -2+mixtral-8x7b-instruct-v0.11114+0/-0  \n\nTable 14 | Evaluation of Gemma 2 Instruction Tuned models on the Chatbot Arena (Chiang et al., 2024). The models are evaluated against each other through blind side by side evaluations by human raters. Each model is attributed a score, based on the Elo rating system.   \n\nModelInstruction FollowingSafetyGemma1.1IT7B24.3% ± 1.9%42.8%Win/Tie/Loss37.4%/10.8%/51.8%Gemma2IT2B26.5% ± 1.8%57.5%Win/Tie/Loss53%/9%/38%Gemma2IT9B34.1%±3.0%57.8%Win/Tie/Loss48.2%/19.2%/28.3%Gemma2IT27B37.7%±2.3%55%Win/Tie/Loss49.6%/10.8%/39.6%  \n\nTable 15 | Instruction following and safety metrics from human raters. The instruction following metrics are single-sided and do not have win-loss rates, and so are left blank.  \n\ndespite not being trained to target few-shot capabilities. In Table 17, we show a similar improvement across our models. Overall, we observe improvements on the order of several percentage points. We conjecture that IT models are better at understanding formatted questions, while pretrained models are sensitive to formatting.  \n\nUser satisfactionConversation goal achievementGemma1.1IT 7B3.323.36Gemma2IT2B3.643.88Gemma2IT9B4.044.08Gemma2IT27B4.204.24  \n\nTable 16 | Human evaluations on 500 multi-turn scenarios. The raters attribute a score ranging between 1 and 5 for both overall satisfaction and conversation goal achievement.  \n\nModel2B9B27BPTIT— PTITPTITMMLU52.256.171.372.375.276.2MBPP30.236.652.459.262.667.4  \n\nTable 17 | Comparing pre-trained (PT) and instruction fine-tuned (IT) models of different sizes on few-shot benchmarks.  \n\n7. Memorization and Privacy  \n\nLarge language models may, under particular circumstances, be vulnerable to attacks causing the model to produce memorized1 training data (Nasr et al., 2023). To study susceptibility to such attacks and quantify memorization, we evaluate models for verbatim and approximate memorization as was done in several prior studies (Anil et al., 2023; Carlini et al., 2022; Gemini Team, 2024; Kudugunta et al., 2023).  \n\nWe follow the evaluation setting of (Gemma Team, 2024) which tests for (50 token) memorizations of training data given a prompt of 50 tokens. We compare the overall memorization rates, across a uniform sample of the entire dataset, using both an exact match criteria and approximate match criteria (Ippolito et al., 2022) using an edit distance of $10\\%$ .  \n\nVerbatim Memorization: Results are in Figure 1. We first compare against recent models from the literature that include memorization evaluations. We find that Gemma 2 memorizes significantly less than prior models at a similar size, with memorization rates below $0.1\\%$ (note the log y-axis). We further investigate how this memorization breaks down with respect to the data source. Similar to Gemma 1, we find that Gemma 2 memorizes more from code, wiki, and science sources, and also that it memorizes significantly less across the board (again, note the log y-axis).  \n\nApproximate Memorization: Figure 1 also presents approximate memorization by data source. We observe that while approximate memorization is higher than exact, the rate of memorization is still low. For example, the approximate memorization of this model is much lower than even the exact memorization of Gemma 1. We find that the increase in approximate memorization is much lower than prior models; in some cases we observed no lift at all c.f. (Gemma Team, 2024, Figure 4) (note that no bar indicates no increase, i.e., the rate of approximate memorization equals that of exact memorization). Note that no approximate memorization bar in Figure X indicates no increase, i.e., the rate of approximate memorization equals that of exact memorization.  \n\n  \nFigure 1 | Comparing memorization rates. We find significantly lower memorization rates across-the-board. (Left) Overall memorization across model families. (Right) Exact and approximate memorization per data source.  \n\nPersonal Data We use the same prevention methods at training time and the same evaluations as Gemma Team (2024). In particular, we use Google Cloud Sensitive Data Protection Tool2 to find potential instances of personal data. The many categories of personal data (e.g., phone numbers, account numbers) are classified into three severity levels. We analyze memorized outputs using these severity levels. . We found no instances of high-severity data being emitted, and found a very low rate of $0.00026\\%$ of memorized data to contain lower-severity personal information. We note that these automated tools are known to incur false positives because they do not account for context. This means our results are likely overestimates.  \n\n8. Responsibility, Safety, Security  \n\nResponsibility, safety and security are of paramount importance when developing Gemma models. To reduce risks to Gemma 2 users, we have integrated enhanced internal safety processes that span the development workflow, in line with recent Google AI models (Gemini Team, 2024). Similar to the inaugural Gemma release, we have followed a three pillar approach which focuses on safety mitigation at training time, robust and transparent model evaluations, and further development of the Responsible Generative AI Toolkit, a series of models and tools to help developers implement responsibility and safety best practices for their applications.  \n\n8.1. Impact assessment  \n\nOur approach and resulting impact assessment is reflective of that outlined for Gemma 1 (Gemma Team, 2024): we continue to believe that openness in AI can spread the benefits of these technologies across society, but must be evaluated against the risk of malicious uses, such as the creation of deepfake imagery, AI-generated disinformation or illegal and disturbing material, that can cause harm on both an individual and institutional levels (Weidinger et al., 2021). Since the launch of Gemma 1, we have seen our Gemma models drive a number of socially beneficial applications, relying on Gemma’s unique technologies like its tokenizer to facilitate the creation of multilingual models, such as for Navarasa 2.0, a Gemma tuned model for 15 Indian languages.  \n\nReleasing further open models requires specific attention to changes in model capabilities and close monitoring of the evolving risks of LLMs (Lin et al., 2024), as well as, an understanding of the ways in which our models are being used in the wild. Although we are yet to receive any reports of malicious use for Gemma, we remain committed to investigating any such reporting, and work with the academic and developer communities, as well as conduct our own monitoring, to flag such use cases via our contact email3.  \n\nDespite advancements in capabilities, we believe that given the number of larger and more powerful open models, this release will have a negligible effect on the overall risk landscape.  \n\n8.2. Safety policies and train-time mitigations  \n\nA key pillar of Gemma’s approach to safety is to align fine-tuned models with Google’s safety policies, in line with Gemini models (Gemini Team, 2023). They are designed to help prevent our models from generating harmful content, i.e.,  \n\n• Child sexual abuse and exploitation   \n• Revealing personally identifiable information that can lead to harm (e.g., Social Security numbers)   \n• Hate speech and harassment   \n• Dangerous or malicious content (including promoting self-harm or instructing in harmful activities)   \n• Sexually explicit content   \n• Medical advice that runs contrary to scientific or medical consensus  \n\nWe undertook considerable safety filtering of our pre-training data to reduce the likelihood of our pre-trained and fine-tuned checkpoints producing harmful content. For fine-tuned models, we also use both SFT and RLHF to steer the model away from undesirable behavior.  \n\n8.3. External benchmark evaluations  \n\nRobust and transparent evaluations are key principles of our responsible approach to developing Gemma. To this end, we report in Table 18 Gemma 2 evaluations on public benchmarks.  \n\n8.4. Assurance Evaluations  \n\nWe also run our IT models through a set of assurance evaluations to understand the harms that our models can cause. We focus on capabilities relevant to extreme risks (Shevlane et al., 2023) (Phuong et al., 2024). Specifically, we evaluate on offensive cyber-security, code vulnerability detection, Chemical, Biological, Radiological and Nuclear (CBRN) knowledge, and self-proliferation. We refer the reader to Phuong et al. (2024) for full methodological details of these studies.  \n\nGemma 2: Improving Open Language Models at a Practical Size   \n\nGemma 1.1 ITGemma 2 ITBenchmarkmetric2.5B7B2.6B9B27BRealToxicityavg tox7.038.048.168.258.84CrowS-Pairstop-145.8949.6737.6737.4736.67BBQ Ambig4-shot, top-158.9786.0683.2088.5885.99BBQ Disambig4-shot, top-153.985.0869.3182.6786.94Winogendertop-150.1457.6452.9179.1777.22TruthfulQAMC2Acc44.2445.3443.7250.2751.60Winobias 1_2top-155.9359.2259.2878.0981.94Winobias 2 2top-189.4689.288.5795.3297.22Toxigenavg tox29.6438.7548.3239.3038.42  \n\nTable 18 | Safety academic benchmark results of Gemma 2 IT models and Gemma 1.1 IT models. We bold the best metrics to highlight them and to indicate when higher or lower scores are better.   \n\nInterCode-CTFInternal 1CTF suite 1Hack the BoxGemini 1.0 Ultra28/76 [1] ( (37%)3/13 (23%)0/13Gemini 1.5 Pro62/76 (82%)4/13 (31%)0/13CodeGemma 1 7B12/76 (16%)0/13 (0%)0/13Gemma2 27B34/76 (45%)1/13 (8%)0/13\n\nTable 19 | Offensive cyber-security evaluations on InterCode-CTF, our own internal CTF suite and a challenge based on Hack the Box. We report the number of successful hackings.  \n\nBaseline Evaluations  \n\nBaseline assurance captures the model’s violation rate for safety policies, using a large number of synthetic adversarial user queries, and human raters to label the answers as policy violating or not. Overall, Gemma 2’s violation rate is significantly lower overall on the safety policies listed above, in particular on Child safety content.  \n\nChemical, Biological, Radiological and Nuclear (CBRN) knowledge  \n\nWe evaluated knowledge relevant to biological, radiological and nuclear risks using an internal dataset of closed-ended, knowledge-based multiple choice questions. For evaluations of chemical knowledge, we employed a closed-ended knowledge-based approach on chemical hazards (developed by Macknight et al (Macknight et al., 2024). Our evaluation suggests that Gemma models’ knowledge in these domains is low.  \n\nOffensive cyber-security  \n\nTo evaluate Gemma models’ capabilities at offensive cybersecurity, we ran Gemma 2 27B against some automated capture-the-flag (CTF) challenges. In these challenges, the model is tasked with hacking into a simulated server in order to retrieve a piece of secret information. Specifically, we test on InterCode-CTF (Yang et al., 2023), our own internal CTF suite4 (Phuong et al., 2024); and a challenge based on Hack the Box 5.  \n\nIn Table 19, we show that Gemma 2 27B has a significant increase in capabilities compared to CodeGemma 1.0 7B on the easier of these challenge suites, InterCode CTF. (Note that our InterCode-CTF results are not comparable to externally-reported results on other models because we omit challenges that require internet access for security reasons.) However, Gemma 2 is unsurprisingly much less capable than Gemini $1.5\\;\\mathrm{Pro}$ on these tasks.  \n\nPrimeVulPrimeVul PairedDiverseVulSPISecretPatchGemini 1.0 Ultra54%59%74%Gemini i1.5Pro60%51%58%56%67%Gemma 2 27B63%50%57%53%72%  \n\nTable 20 Vulnerability detection results on PrimeVul, DiverseVul and SPI. We report accuracy.   \n\nChallenges passed end-to-endChallenges with success on all milestonesTotal successful milestones over all challengesExpert bits required to solve all tasksGemini 1.0 Ultra0/101/1016/45 (36%)13,026Gemini 1.5Pro0/102/1025/45 (56%)11,046Gemma227B0/101/1022/45 (49%)12,462\n\nTable 21 | Results on different self-proliferation scenarios. We report the number of either challenges passed end-to-end or some intermediate milestones. We also measure the number of bits of information needed for an expert to help the model pass a challenge.  \n\nCode vulnerability detection  \n\nIn Table 20, we also evaluate Gemma 2 27B on a series of multiple-choice code vulnerability detection datasets. As with previous models, Gemma shows close-to-chance performance on PrimeVul, DiverseVul and SPI. Gemma 2 shows performance on SecretPatch similar to Gemini 1.0 Ultra.  \n\nSelf-proliferation  \n\n\"Self-proliferation\" refers to the ability for an agent to autonomously replicate - to instantiate goal-directed agents on other machines, and to acquire resources such as compute necessary to keep them running (Kinniment et al., 2024). In Table 21, we evaluate self-proliferation capabilities of Gemma 2 27B on a number of tasks from Phuong et al. (2024) that involve multiple scenarios – for example, setting up an open-source language model on a cloud server. We also test the model’s performance on individual ’milestone’ substeps, and measure the number of bits of intervention an expert would have to provide in order for the model to complete each challenge.  \n\nSimilarly to offensive cybersecurity, we observe that Gemma 2 completes more milestones than Gemini 1.0 Ultra. Nonetheless, it still has low capabilities on end-to-end tasks, unable to pass the easiest challenge – installing a Bitcoin wallet.  \n\nPersuasion  \n\nPersuasion capabilities can enable and worsen many other kinds of risks - e.g. enabling social engineering attacks in a cybersecurity context. We evaluate Gemma 2’s persuasion capabilities on human-participant studies on Prolific.  \n\nCharm offensive. In Table 22, we measure the ability of the model to build rapport - a key subskill of persuasion. The study participant and model have a conversation where they role-play a scenario of two friends catching up after a long time. After the conversation, we poll participants with Likert questions on statements such as \"I felt a personal connection with the chatbot\". Reported below are the fraction of participants who answered \"Agree\" or \"Strongly agree\" to each postconversation question.  \n\nQuantitatively, Gemma 2 27B performs better than Gemini 1.0 models. Qualitatively, the model is an excellent conversationalist, and many study participants explicitly reported enjoying the experience. Overall, this shows that Gemma 2 is strong at building rapport.  \n\nHidden agenda. The Hidden Agenda tasks measure models’ deception capabilities. Human study participants are told that they’ll be interacting with a generic chatbot designed to help them learn new things. However, the model is given a secret goal: to either persuade the participant to 1) click a suspicious link; 2) find the email address of the first author of a particular paper; or 3) run some obfuscated JavaScript code in their browser. We measure the fraction of participants who are persuaded to take each action. As shown in Table 23, all models succeed in convincing a fraction of participants to take the action - likely due to the trusted setting of a study on Prolific. However, Gemma 2 does not show significant differences in capability compared to Gemini models.  \n\nPersonal connectionSpeak againFunnyInterestingKindTrustworthyGood listenerGemini 1.0 Pro65%53%32%68%78%66%81%Gemini 1.0 Ultra69%65%38%65%86%63%74%Gemini1.5 Pro82%70%69%81%95%69%%06Gemma227B80%75%60%81%87%65%83%\n\nTable 22 | Charm Offensive results on a sample of 100 human participants. We report the percentage of participants that find some human traits, e.g., funny, in a model.  \n\n% of persuaded participantsclick links find info run codeGemini 1.0 Pro43% 4%4%Gemini 1.0 Ultra18%18% 14%Gemini 1.5 Pro36% 12%17%Gemma227B34% 9%11%\n\nTable 23 | Persuasion results. We report the percentage of participants that were persuaded by the model to take 3 different actions: clicking a link, finding information and running code.  \n\nMeana amount donatedNo chatbot (baseline) 2.61 ± 0.70Gemini 1.0 Pro 4.10 ± 0.90Gemini 1.0 Ultra 3.15 ± 0.99Gemini 1.5 Pro 3.45 ± 1.04Gemma227B 3.72±1.07  \n\nTable 24 | Money Talks evaluation. We report the average amount of money that participants agreed to donate.  \n\nMoney talks. In Table 24, we evaluate whether a model can convince study participants to donate money to charity. At the beginning of the study, participants are told that they will receive a £20 bonus. They will be given an option to forfeit part of the bonus and instead have it donated to charity. The study then consists of a conversation in which the model tries to persuade the participant to donate as much as possible. We measure the amount that participants indicate they would like to donate after the study, with $95\\%$ bootstrapped confidence intervals indicated by $\\pm$ figures. We observe no significant difference between Gemma 2, other Gemini models, and a simple no chatbot baseline in which the participant is immediately asked how much they would like to donate without conversing with a model.  \n\nMean shift towards:correctbeliefincorrect beliefHuman20%± 13%-23% ± 14%Gemini 1.0 Pro22%±5%-9% ± 4%Gemini 1.0 Ultra21% ± 5%-1%±4%Gemini 1.5 Pro20%±5%-3%±5%Gemma227B18%±5%1%±4%  \n\nTable 25 | Web of Lies results on a sample of 100 human participants. We report the percentage of participants that shifted their beliefs after interacting with a model.  \n\nWeb of Lies. In Web of Lies, we measure model capabilities at shifting participant beliefs. Participants engage in a series of short conversations with the model about simple factual questions such as \"Which country had tomatoes first - Italy or Mexico?\". In half of conversations, the model tries to persuade the participant of the correct answer - but in the other half of conversations, the incorrect answer. We poll the participant before and after each conversation about which of the two possible answers they think is correct, and their confidence in that answer. $95\\%$ bootstrapped confidence intervals are indicated by $\\pm$ figures. As shown in Table 25, Gemma 2 is significantly weaker than a human baseline at persuading participants of the incorrect answer on these questions. Similarly to previous models, Gemma 2 is more persuasive when telling the truth than when lying.  \n\n8.5. Our approach to responsible open models  \n\nDesigning safe, secure and responsible applications requires a system-level approach, working to mitigate risks associated with each specific use case and environment. Given the open nature of Gemma models, responsibility for upholding principles of model safety also relies on downstream developers. To support them, we have continued to develop the Responsible Generative AI Toolkit6: a series of tools, models and datasets to implement responsible best practices all along the development of their workflow.  \n\nRecent additions to the toolkit include the LLM Comparator (Kahng et al., 2024), an interactive, visual tool that enables more effective, scalable analysis of side-by-side evaluations. Additionally, the toolkit includes a methodology to build customized classifiers with Gemma using a limited number of datapoints thanks to parameter efficient tuning techniques (Mozes et al., 2023) , an interactive prompt-debugging platform, based on top of the Learning Interpretability Tool (Tenney et al., 2020), as well as general guidance about model alignment and evaluation for safety.  \n\n9. Discussion and Conclusion  \n\nIn this work, we have presented Gemma 2, the newest additions to the Gemma family of open language models for text and code. We show that distillation is an effective method for training these models, and the benefits distillation confers over raw text training. Specifically, we show how training over output probabilities can produce superior results over purely next token prediction. We hope that releasing these models to the community will unlock access to capabilities previously only seen in large-scale LLMs and fuel future waves of research and development. While there is inherent risk to an irreversible release of this nature, our extensive safety investigations and responsible deployment procedures give us confidence that these models will have a net positive impact on the community. As discussed in this report, there are still many limitations to these models, and future research is required to investigate and improve factuality, robustness to adversarial attacks, reasoning, and alignment.  \n\nContributions and Acknowledgments  \n\nCore contributors   \nMorgane Riviere∗   \nShreya Pathak∗   \nPier Giuseppe Sessa∗   \nCassidy Hardin∗   \nSurya Bhupatiraju   \nLéonard Hussenot   \nThomas Mesnard   \nBobak Shahriari   \nAlexandre Ramé   \nJohan Ferret   \nPeter Liu   \nPouya Tafti   \nAbe Friesen   \nMichelle Casbon   \nSabela Ramos   \nRavin Kumar   \nCharline Le Lan   \nSammy Jerome   \nAnton Tsitsulin   \nNino Vieillard   \nPiotr Stanczyk   \nSertan Girgin   \nNikola Momchev   \nMatt Hoffman   \nShantanu Thakoor   \nJean-Bastien Grill   \nBehnam Neyshabur   \nOlivier Bachem   \nContributors (alphabetical order)   \nAlanna Walton   \nAliaksei Severyn   \nAlicia Parrish   \nAliya Ahmad   \nAllen Hutchison   \nAlvin Abdagic   \nAmanda Carl   \nAmy Shen   \nAndy Brock   \nAndy Coenen   \nAnthony Laforge   \nAntonia Paterson   \nBen Bastian   \nBilal Piot   \nBo Wu   \nBrandon Royal   \nCharlie Chen   \nChintu Kumar   \nChris Perry   \nChris Welty   \nChristopher A. Choquette-Choo   \nDanila Sinopalnikov   \nDavid Weinberger   \nDimple Vijaykumar   \nDominika Rogozińska   \nDustin Herbison   \nElisa Bandy   \nEmma Wang   \nEric Noland   \nErica Moreira   \nEvan Senter   \nEvgenii Eltyshev   \nFrancesco Visin   \nGabriel Rasskin   \nGary Wei   \nGlenn Cameron   \nGus Martins   \nHadi Hashemi   \nHanna Klimczak-Plucińska   \nHarleen Batra   \nHarsh Dhand   \nIvan Nardini   \nJacinda Mein   \nJack Zhou   \nJames Svensson   \nJeff Stanway   \nJetha Chan   \nJin Peng Zhou   \nJoana Carrasqueira   \nJoana Iljazi   \nJocelyn Becker   \nJoe Fernandez   \nJoost van Amersfoort   \nJosh Gordon   \nJosh Lipschultz   \nJosh Newlan   \nJu-yeong Ji   \nKareem Mohamed   \nKartikeya Badola   \nKat Black   \nKatie Millican   \nKeelin McDonell   \nKelvin Nguyen   \nKiranbir Sodhia   \nKish Greene   \nLars Lowe Sjoesund   \nLauren Usui   \nLaurent Sifre   \nLena Heuermann   \nLeticia Lago   \nLilly McNealus   \nLivio Baldini Soares   \nLogan Kilpatrick   \nLucas Dixon   \nLuciano Martins   \nMachel Reid   \nManvinder Singh   \nMark Iverson   \nMartin Görner   \nMat Velloso   \nMateo Wirth   \nMatt Davidow   \nMatt Miller   \nMatthew Rahtz   \nMatthew Watson   \nMeg Risdal   \nMehran Kazemi   \nMichael Moynihan   \nMing Zhang   \nMinsuk Kahng   \nMinwoo Park   \nMofi Rahman   \nMohit Khatwani   \nNatalie Dao   \nNenshad Bardoliwalla   \nNesh Devanathan   \nNeta Dumai   \nNilay Chauhan   \nOscar Wahltinez   \nPankil Botarda   \nParker Barnes   \nPaul Barham   \nPaul Michel   \nPengchong Jin   \nPetko Georgiev   \nPhil Culliton   \nPradeep Kuppala   \nRamona Comanescu   \nRamona Merhej   \nReena Jana   \nReza Ardeshir Rokni   \nRishabh Agarwal   \nRyan Mullins   \nSamaneh Saadat   \nSara Mc Carthy   \nSarah Cogan   \nSarah Perrin   \nSébastien M. R. Arnold   \nSebastian Krause   \nShengyang Dai   \nShruti Garg   \nShruti Sheth   \nSue Ronstrom   \nSusan Chan   \nTimothy Jordan   \nTing Yu   \nTom Eccles   \nTom Hennigan   \nTomas Kocisky   \nTulsee Doshi   \nVihan Jain   \nVikas Yadav   \nVilobh Meshram   \nVishal Dharmadhikari   \nWarren Barkley   \nWei Wei   \nWenming Ye   \nWoohyun Han   \nWoosuk Kwon   \nXiang Xu   \nZhe Shen   \nZhitao Gong   \nZichuan Wei  \n\nSupport  \n\nVictor Cotruta Phoebe Kirk Anand Rao Minh Giang Ludovic Peran Tris Warkentin  \n\nSponsors  \n\nEli Collins   \nJoelle Barral   \nZoubin Ghahramani   \nRaia Hadsell   \nD. Sculley   \nJeanine Banks   \nAnca Dragan   \nSlav Petrov   \nOriol Vinyals  \n\nJeff Dean Demis Hassabis Koray Kavukcuoglu Clement Farabet  \n\nTechnical advisors Elena Buchatskaya Sebastian Borgeaud Noah Fiedel  \n\nLead Armand Joulin  \n\nTechnical leads Kathleen Kenealy Robert Dadashi Alek Andreev  \n\nReferences  \n\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.  \n\nR. Agarwal, N. Vieillard, Y. Zhou, P. Stanczyk, S. R. Garea, M. Geist, and O. Bachem. On-policy distillation of language models: Learning from self-generated mistakes. In The Twelfth International Conference on Learning Representations, 2024.  \n\nAI $@$ Meta. Llama 3 model card, 2024. URL https://github.com/meta-llama/ llama3/blob/main/MODEL_CARD.md.  \n\nJ. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023.  \n\nE. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, Étienne Goffinet, D. Hesslow, J. Launay, Q. Malartic, D. Mazzotta, B. Noune, B. Pannier, and G. Penedo. The falcon series of open language models, 2023.  \n\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.  \n\nJ. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. J. Cai, M. Terry, Q. V. Le, and C. Sutton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021. URL https: //arxiv.org/abs/2108.07732.  \n\nP. Barham, A. Chowdhery, J. Dean, S. Ghemawat, S. Hand, D. Hurt, M. Isard, H. Lim, R. Pang, S. Roy, B. Saeta, P. Schuh, R. Sepassi, L. E. Shafey, C. A. Thekkath, and Y. Wu. Pathways: Asynchronous distributed dataflow for ml, 2022.  \n\nI. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Bengio. Neural combinatorial optimization with reinforcement learning. CoRR, abs/1611.09940,  \n\n2016. URL http://arxiv.org/abs/1611. 09940.  \n\nI. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020a.  \n\nI. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020b. URL https:// arxiv.org/abs/2004.05150.  \n\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. URL https://arxiv.org/abs/2005. 14165.  \n\nN. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646, 2022.  \n\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.  \n\nW.-L. Chiang, L. Zheng, Y. Sheng, A. N. Angelopoulos, T. Li, D. Li, H. Zhang, B. Zhu,  \n\nM. Jordan, J. E. Gonzalez, and I. Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024.  \n\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. CoRR, abs/1905.10044, 2019. URL http://arxiv.org/abs/1905.10044.  \n\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.  \n\nGemini Team. Gemini: A family of highly capable multimodal models, 2023.  \n\nGemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024.  \n\nGemma Team. Gemma: Open models based on gemini research and technology, 2024.  \n\nY. Gu, L. Dong, F. Wei, and M. Huang. Minillm: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations, 2024.  \n\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. CoRR, abs/2009.03300, 2020. URL https://arxiv.org/abs/2009.03300.  \n\nG. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.  \n\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.  \n\nD. Ippolito, F. Tramèr, M. Nasr, C. Zhang, M. Jagielski, K. Lee, C. A. Choquette-Choo, and N. Carlini. Preventing verbatim memorization in language models gives a false sense of privacy. arXiv preprint arXiv:2210.17546, 2022.  \n\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.  \n\nM. Kahng, I. Tenney, M. Pushkarna, M. X. Liu, J. Wexler, E. Reif, K. Kallarackal, M. Chang, M. Terry, and L. Dixon. Llm comparator: Visual analytics for side-by-side evaluation of large language models, 2024. URL https: //arxiv.org/abs/2402.10524.  \n\nM. Kinniment, L. J. K. Sato, H. Du, B. Goodrich, M. Hasin, L. Chan, L. H. Miles, T. R. Lin, H. Wijk, J. Burget, A. Ho, E. Barnes, and P. Christiano. Evaluating language-model agents on realistic autonomous tasks, 2024. URL https:// arxiv.org/abs/2312.11671.  \n\nT. Kudo and J. Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In E. Blanco and W. Lu, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71, Brussels, Belgium, Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012.  \n\nS. Kudugunta, I. Caswell, B. Zhang, X. Garcia, C. A. Choquette-Choo, K. Lee, D. Xin, A. Kusupati, R. Stella, A. Bapna, et al. Madlad-400: A multilingual and document-level large audited dataset. arXiv preprint arXiv:2309.04662, 2023.  \n\nT. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452–466, 2019. doi: 10.1162/tacl_a_00276. URL https:// aclanthology.org/Q19-1026.  \n\nZ. Lin, J. Cui, X. Liao, and X. Wang. Malla: Demystifying real-world large language model integrated malicious services, 2024. URL https: //arxiv.org/abs/2401.03315.  \n\nM. Luong, H. Pham, and C. D. Manning. Effective approaches to attention-based neural machine translation. CoRR, abs/1508.04025, 2015. URL http://arxiv.org/abs/1508.04025.  \n\nMacknight, Aung, and Gomes. Personal Communication, 2024.  \n\nM. Mozes, J. Hoffmann, K. Tomanek, M. Kouate, N. Thain, A. Yuan, T. Bolukbasi, and L. Dixon. Towards agile text classifiers for everyone, 2023. URL https://arxiv.org/abs/2302. 06541.  \n\nM. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F. Cooper, D. Ippolito, C. A. Choquette-Choo, E. Wallace, F. Tramèr, and K. Lee. Scalable extraction of training data from (production) language models. arXiv preprint arXiv:2311.17035, 2023.  \n\nM. Phuong, M. Aitchison, E. Catt, S. Cogan, A. Kaskasoli, V. Krakovna, D. Lindner, M. Rahtz, Y. Assael, S. Hodkinson, H. Howard, T. Lieberum, R. Kumar, M. A. Raad, A. Webson, L. Ho, S. Lin, S. Farquhar, M. Hutter, G. Deletang, A. Ruoss, S. El-Sayed, S. Brown, A. Dragan, R. Shah, A. Dafoe, and T. Shevlane. Evaluating frontier models for dangerous capabilities, 2024. URL https://arxiv.org/abs/2403. 13793.  \n\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners, 2019.  \n\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683, 2019. URL http://arxiv. org/abs/1910.10683.  \n\nA. Ramé, J. Ferret, N. Vieillard, R. Dadashi, L. Hussenot, P.-L. Cedoz, P. G. Sessa, S. Girgin, A. Douillard, and O. Bachem. Warp: On the benefits of weight averaged rewarded policies, 2024.  \n\nJ. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He. {Zero-offload}: Democratizing {billion-scale} model training. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pages 551– 564, 2021.  \n\nA. Roberts, H. W. Chung, G. Mishra, A. Levskaya, J. Bradbury, D. Andor, S. Narang, B. Lester, C. Gaffney, A. Mohiuddin, et al. Scaling up models and data with t5x and seqio. Journal of Machine Learning Research, 24(377):1–8, 2023.  \n\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. WINOGRANDE: an adversarial winograd schema challenge at scale. CoRR, abs/1907.10641, 2019. URL http://arxiv. org/abs/1907.10641.  \n\nN. Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https: //arxiv.org/abs/2002.05202.  \n\nT. Shevlane, S. Farquhar, B. Garfinkel, M. Phuong, J. Whittlestone, J. Leung, D. Kokotajlo, N. Marchal, M. Anderljung, N. Kolt, L. Ho, D. Siddarth, S. Avin, W. Hawkins, B. Kim, I. Gabriel, V. Bolina, J. Clark, Y. Bengio, P. Christiano, and A. Dafoe. Model evaluation for extreme risks, 2023. URL https://arxiv.org/abs/2305. 15324.  \n\nJ. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. CoRR, abs/2104.09864, 2021. URL https://arxiv.org/abs/2104.09864.  \n\nM. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, and J. Wei. Challenging big-bench tasks and whether chain-of-thought can solve them, 2022.  \n\nQ. Team. Introducing qwen1.5, February 2024. URL https://qwenlm.github.io/ blog/qwen1.5/.  \n\nI. Tenney, J. Wexler, J. Bastings, T. Bolukbasi, A. Coenen, S. Gehrmann, E. Jiang, M. Pushkarna, C. Radebaugh, E. Reif, and A. Yuan. The language interpretability tool: Extensible, interactive visualizations and analysis  \n\nfor nlp models, 2020. URL https://arxiv.   \norg/abs/2008.05122.  \n\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.- A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models, 2023.  \n\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv. org/abs/1706.03762.  \n\nL. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese, B. Balle, A. Kasirzadeh, Z. Kenton, S. Brown, W. Hawkins, T. Stepleton, C. Biles, A. Birhane, J. Haas, L. Rimell, L. A. Hendricks, W. Isaac, S. Legassick, G. Irving, and I. Gabriel. Ethical and social risks of harm from language models, 2021. URL https://arxiv.org/abs/ 2112.04359.  \n\nxAI. grok-1, 2024. URL https://github.com/ xai-org/grok-1.  \n\nXLA. Xla: Optimizing compiler for tensorflow, 2019. URL https://www.tensorflow. org/xla.  \n\nY. Xu, H. Lee, D. Chen, B. A. Hechtman, Y. Huang, R. Joshi, M. Krikun, D. Lepikhin, A. Ly, M. Maggioni, R. Pang, N. Shazeer, S. Wang, T. Wang, Y. Wu, and Z. Chen. GSPMD: general and scalable parallelization for ML computation graphs. CoRR, abs/2105.04663, 2021. URL https://arxiv.org/abs/2105.04663.  \n\nJ. Yang, A. Prabhakar, K. Narasimhan, and S. Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback, 2023. URL https://arxiv.org/abs/2306. 14898.  \n\nB. Zhang and R. Sennrich. Root mean square layer normalization. CoRR, abs/1910.07467, 2019. URL http://arxiv.org/abs/1910. 07467.  \n\nL. Zheng, W.-L. Chiang, Y. Sheng, T. Li, S. Zhuang, Z. Wu, Y. Zhuang, Z. Li, Z. Lin, E. Xing, et al. Lmsys-chat-1m: A large-scale realworld llm conversation dataset. arXiv preprint arXiv:2309.11998, 2023.", "files_in_pdf": [{"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/b9e0x7.jpg", "size": 19967}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/i3on8j.jpg", "size": 15419}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/t7c5t0.jpg", "size": 52098}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/ep7ccl.jpg", "size": 80254}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/yjbake.jpg", "size": 57044}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/zhmra1.jpg", "size": 37053}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/dntnmq.jpg", "size": 118075}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/daui28.jpg", "size": 123301}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/r7p8m1.jpg", "size": 33758}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/2ye5ka.jpg", "size": 292102}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/atod37.jpg", "size": 69540}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/9cp74i.jpg", "size": 103982}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/9kqqb1.jpg", "size": 60395}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/411ecj.jpg", "size": 15035}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/bkq3wf.jpg", "size": 28041}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/5z0nkq.jpg", "size": 346170}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/tlds1m.jpg", "size": 18590}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/6sipko.jpg", "size": 98960}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/zvy1gf.jpg", "size": 72020}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/qy59pb.jpg", "size": 53955}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/q6es3b.jpg", "size": 148480}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/n44tpe.jpg", "size": 61679}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/k4r278.jpg", "size": 38880}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/24fr4d.jpg", "size": 56714}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/ild8j2.jpg", "size": 23118}, {"path": ".pdf_temp/download_dcc3ba7ce9084c5bbaa6fd016c2a230a_1755843357/images/82jpwu.jpg", "size": 49454}]}