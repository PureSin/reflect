{
  "_source_id": 8,
  "sources": {
    "WebLLM-Compatible Models for Sentiment Analysis Research": {
      "https://github.com/mlc-ai/web-llm": {
        "id": 1,
        "publisher": "MLC AI",
        "url": "https://github.com/mlc-ai/web-llm",
        "title": "WebLLM: High-Performance In-Browser LLM Inference Engine",
        "info": "Comprehensive framework documentation for WebLLM, including technical specifications, supported models, WebGPU requirements, and deployment characteristics for browser-based AI applications"
      },
      "https://webllm.mlc.ai/": {
        "id": 2,
        "publisher": "WebLLM",
        "url": "https://webllm.mlc.ai/",
        "title": "WebLLM Official Documentation",
        "info": "Official WebLLM documentation covering framework capabilities, OpenAI API compatibility, and browser deployment features"
      },
      "https://huggingface.co/meta-llama/Llama-3.2-1B": {
        "id": 3,
        "publisher": "Meta",
        "url": "https://huggingface.co/meta-llama/Llama-3.2-1B",
        "title": "Llama 3.2 1B Model Card",
        "info": "Complete technical specifications for Llama 3.2 1B model including performance benchmarks, memory requirements, quantization schemes, and sentiment analysis relevant metrics"
      },
      "https://huggingface.co/blog/smollm": {
        "id": 4,
        "publisher": "Hugging Face",
        "url": "https://huggingface.co/blog/smollm",
        "title": "SmolLM: State-of-the-art Small Language Models",
        "info": "Comprehensive analysis of SmolLM family (135M, 360M, 1.7B parameters) with technical specifications, training methodology, performance benchmarks, and WebGPU deployment characteristics"
      },
      "https://arxiv.org/html/2503.03225v1": {
        "id": 5,
        "publisher": "arXiv/Harbin Institute of Technology",
        "url": "https://arxiv.org/html/2503.03225v1",
        "title": "Targeted Distillation for Sentiment Analysis",
        "info": "Comprehensive research on targeted distillation for sentiment analysis featuring SentiBench benchmark with detailed performance comparisons of small language models (TinyLlama, Phi-2, Qwen2.5-1.5B, Gemma-2) on sentiment analysis tasks with specific F1-scores and capabilities assessment"
      },
      "https://medium.com/@zaiinn440/best-slm-stable-lm-tiny-llama-mini-cpm-and-qwen-1-5-91134cfddbc3": {
        "id": 6,
        "publisher": "Medium/@zaiinn440",
        "url": "https://medium.com/@zaiinn440/best-slm-stable-lm-tiny-llama-mini-cpm-and-qwen-1-5-91134cfddbc3",
        "title": "Best SLM Comparison: StableLM vs TinyLlama vs MiniCPM vs Qwen",
        "info": "Comparative analysis of Small Language Models (Stable LM-2 1.6B, Tiny LlaMA 1.1B, QWEN-1.5 1.8B, MiniCPM-2B) with specific focus on emotional intelligence evaluation, text analysis capabilities, and performance ratings across multiple NLP tasks"
      },
      "https://huggingface.co/microsoft/Phi-3.5-mini-instruct": {
        "id": 7,
        "publisher": "Microsoft",
        "url": "https://huggingface.co/microsoft/Phi-3.5-mini-instruct",
        "title": "Phi-3.5 Mini Instruct Model Card",
        "info": "Detailed technical specifications for Phi-3.5 Mini Instruct model including architecture, performance benchmarks, multilingual capabilities, memory requirements, and instruction following capabilities with 128K context length"
      },
      "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct": {
        "id": 8,
        "publisher": "Alibaba Cloud",
        "url": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct",
        "title": "Qwen2.5-1.5B-Instruct Model Documentation",
        "info": "Complete specifications for Qwen2.5-1.5B-Instruct model covering architecture details, structured output capabilities, context length support, and performance improvements over previous versions"
      }
    }
  }
}